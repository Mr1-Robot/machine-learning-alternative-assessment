{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mr1-Robot/machine-learning-alternative-assessment/blob/main/alternative_assessment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Alternative Assessment - WOA7015\n",
        "#####MUAAMAR MOHAMMED ABDULLAH AL-GHRAIRI - 24084470"
      ],
      "metadata": {
        "id": "d3YMC15UZMvQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8KlzI69XiO5"
      },
      "outputs": [],
      "source": [
        "# Mount google drive to access the dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DATA_ROOT = \"/content/drive/MyDrive/alternative-assessment-materials/\"\n",
        "IMAGE_DIR = f\"{DATA_ROOT}VQA_RAD_Images\"\n",
        "ANNOTATION_FILE = f\"{DATA_ROOT}VQA_RAD_Dataset.json\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Core libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Data processing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Utilities\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"-\"*30)\n",
        "print(\"SETUP\")\n",
        "print(\"-\"*30)\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "print(\"-\"*30)"
      ],
      "metadata": {
        "id": "TF88Uj5tYtHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load & Explore **Dataset**"
      ],
      "metadata": {
        "id": "lDH11gZfdFB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "vqa_data = pd.read_json(ANNOTATION_FILE)\n",
        "\n",
        "print(f\"Total Q&A pairs: {len(vqa_data)}\")\n",
        "print(f\"Columns: {vqa_data.columns.tolist()}\")\n",
        "\n",
        "# Answer type distribution\n",
        "answer_types = vqa_data['answer_type'].value_counts()\n",
        "print(\"\\nAnswer Type Distribution:\")\n",
        "for k, v in answer_types.items():\n",
        "    print(f\"{k}: {v} ({v/len(vqa_data)*100:.1f}%)\")\n",
        "\n",
        "# Closed-ended questions\n",
        "closed_data = vqa_data[vqa_data['answer_type'] == 'CLOSED'].copy()\n",
        "print(f\"\\nClosed-ended questions: {len(closed_data)}\")\n",
        "\n",
        "# Normalize answers\n",
        "closed_data['answer_norm'] = closed_data['answer'].str.lower().str.strip()\n",
        "\n",
        "# Yes/No distribution\n",
        "yn_dist = closed_data['answer_norm'].value_counts()\n",
        "print(\"\\nYes/No Distribution:\")\n",
        "for ans, count in yn_dist.items():\n",
        "    print(f\"{ans}: {count} ({count/len(closed_data)*100:.1f}%)\")\n",
        "\n",
        "# Organ distribution\n",
        "organ_dist = vqa_data['image_organ'].value_counts()\n",
        "print(\"\\nImage Organ Distribution:\")\n",
        "for organ, count in organ_dist.items():\n",
        "    print(f\"{organ}: {count} ({count/len(vqa_data)*100:.1f}%)\")"
      ],
      "metadata": {
        "id": "g8DtBq2-bgik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Constructing Binary (Yes/No) Dataset**"
      ],
      "metadata": {
        "id": "gTdbqQRwCIls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"-\"*30)\n",
        "print(\"Constructing Binary (Yes/No) Dataset\")\n",
        "print(\"-\"*30)\n",
        "\n",
        "# 1. Filter CLOSED-ended questions\n",
        "closed_data = vqa_data[vqa_data['answer_type'].str.strip() == 'CLOSED'].copy()\n",
        "\n",
        "# 2. Normalize answers\n",
        "closed_data['answer_norm'] = closed_data['answer'].str.lower().str.strip()\n",
        "\n",
        "# 3. Keep ONLY true binary answers\n",
        "binary_data = closed_data[closed_data['answer_norm'].isin(['yes', 'no'])].copy()\n",
        "\n",
        "print(f\"Total CLOSED questions: {len(closed_data)}\")\n",
        "print(f\"Binary Yes/No questions: {len(binary_data)}\")\n",
        "\n",
        "# 4. Map labels explicitly\n",
        "binary_data['label'] = binary_data['answer_norm'].map({'no': 0, 'yes': 1})\n",
        "\n",
        "print(f\"Yes: {binary_data['label'].sum()}, \"\n",
        "      f\"No: {len(binary_data) - binary_data['label'].sum()}\")\n",
        "\n",
        "# 5. Set random seed\n",
        "np.random.seed(42)\n",
        "\n",
        "# 6. Train / Test split (15% test)\n",
        "train_val, test = train_test_split(\n",
        "    binary_data,\n",
        "    test_size=0.15,\n",
        "    random_state=42,\n",
        "    stratify=binary_data['label']\n",
        ")\n",
        "\n",
        "# 7. Train / Val split (15% of total)\n",
        "train, val = train_test_split(\n",
        "    train_val,\n",
        "    test_size=0.176,  # 0.15 / 0.85\n",
        "    random_state=42,\n",
        "    stratify=train_val['label']\n",
        ")\n",
        "\n",
        "# 8. Reset indices\n",
        "train = train.reset_index(drop=True)\n",
        "val = val.reset_index(drop=True)\n",
        "test = test.reset_index(drop=True)\n",
        "\n",
        "print(\"\\nSplit sizes:\")\n",
        "print(f\"  Train: {len(train)} ({len(train)/len(binary_data)*100:.1f}%)\")\n",
        "print(f\"  Val:   {len(val)} ({len(val)/len(binary_data)*100:.1f}%)\")\n",
        "print(f\"  Test:  {len(test)} ({len(test)/len(binary_data)*100:.1f}%)\")\n",
        "\n",
        "print(\"\\nClass distribution:\")\n",
        "print(f\"  Train - Yes: {train['label'].sum()}, No: {len(train)-train['label'].sum()}\")\n",
        "print(f\"  Val   - Yes: {val['label'].sum()}, No: {len(val)-val['label'].sum()}\")\n",
        "print(f\"  Test  - Yes: {test['label'].sum()}, No: {len(test)-test['label'].sum()}\")\n",
        "\n",
        "print(\"-\"*30)\n",
        "\n",
        "# 9. Save split IDs (stable identifiers)\n",
        "os.makedirs(f\"{DATA_ROOT}\", exist_ok=True)\n",
        "\n",
        "split_ids = {\n",
        "    'train_qids': train['qid'].tolist(),\n",
        "    'val_qids': val['qid'].tolist(),\n",
        "    'test_qids': test['qid'].tolist()\n",
        "}\n",
        "\n",
        "with open(f\"{DATA_ROOT}split_qids.json\", \"w\") as f:\n",
        "    json.dump(split_ids, f)\n",
        "\n",
        "print(f\"Split QIDs saved to: {DATA_ROOT}split_qids.json\")"
      ],
      "metadata": {
        "id": "yyt08zipd5Ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Create PyTorch Dataset**"
      ],
      "metadata": {
        "id": "MSyi-Qehitho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "class VQADataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset returning:\n",
        "      - image tensor\n",
        "      - raw question string\n",
        "      - binary label\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataframe, image_dir, transform=None):\n",
        "        self.data = dataframe.reset_index(drop=True)\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "\n",
        "        # Load image\n",
        "        image_path = os.path.join(self.image_dir, row['image_name'])\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Raw question (string)\n",
        "        question = row['question']\n",
        "\n",
        "        # Binary label\n",
        "        label = torch.tensor(row['label'], dtype=torch.long)\n",
        "\n",
        "        return image, question, label\n",
        "\n",
        "\n",
        "# Image Transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),           # Standard for ResNet\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],           # ImageNet stats\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "# Create Dataset Objects\n",
        "train_dataset = VQADataset(train, IMAGE_DIR, transform=transform)\n",
        "val_dataset   = VQADataset(val, IMAGE_DIR, transform=transform)\n",
        "test_dataset  = VQADataset(test, IMAGE_DIR, transform=transform)\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(\"Datasets Created\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Val samples:   {len(val_dataset)}\")\n",
        "print(f\"Test samples:  {len(test_dataset)}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# One Sample\n",
        "sample_image, sample_question, sample_label = train_dataset[0]\n",
        "\n",
        "print(\"Sample item check:\")\n",
        "print(f\"  Image shape: {sample_image.shape}\")\n",
        "print(f\"  Question: {sample_question}\")\n",
        "print(f\"  Label: {sample_label.item()}\")\n",
        "print(\"-\" * 30)"
      ],
      "metadata": {
        "id": "pYnkjfA6ikFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Dataloaders**"
      ],
      "metadata": {
        "id": "0-eiM7zllneT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,        # Shuffle only training data\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(\"Dataloaders Created\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"Train batches: {len(train_loader)} (batch size = {BATCH_SIZE})\")\n",
        "print(f\"Val batches:   {len(val_loader)}\")\n",
        "print(f\"Test batches:  {len(test_loader)}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Inspect One Batch\n",
        "sample_batch = next(iter(train_loader))\n",
        "batch_images, batch_questions, batch_labels = sample_batch\n",
        "\n",
        "print(\"Sample batch inspection:\")\n",
        "print(f\"  Images tensor shape: {batch_images.shape}\")      # [B, 3, 224, 224]\n",
        "print(f\"  Number of questions: {len(batch_questions)}\")    # B\n",
        "print(f\"  Example question: {batch_questions[0]}\")\n",
        "print(f\"  Labels tensor shape: {batch_labels.shape}\")      # [B]\n",
        "print(\"-\" * 30)"
      ],
      "metadata": {
        "id": "5Ykh0IPykeNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**CNN Model Architecture (ResNet18)**"
      ],
      "metadata": {
        "id": "hGezVSwyntdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "\n",
        "class CNNBaseline(nn.Module):\n",
        "    def __init__(self, num_classes=2, dropout=0.3, fine_tune=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.backbone = models.resnet18(\n",
        "            weights=models.ResNet18_Weights.IMAGENET1K_V1\n",
        "        )\n",
        "\n",
        "        # Freeze backbone initially\n",
        "        for param in self.backbone.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Optionally unfreeze last block for fine-tuning\n",
        "        if fine_tune:\n",
        "            for param in self.backbone.layer4.parameters():\n",
        "                param.requires_grad = True\n",
        "\n",
        "        # Replace classification head\n",
        "        in_features = self.backbone.fc.in_features\n",
        "        self.backbone.fc = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(in_features, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, images):\n",
        "        return self.backbone(images)"
      ],
      "metadata": {
        "id": "gtXIz1oLmdmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Training & Evaulation functions**"
      ],
      "metadata": {
        "id": "Chcin3-Hoyi2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    \"\"\"Train CNN baseline for one epoch (vision-only).\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, _, labels in tqdm(dataloader, desc=\"Training\"):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass (image only)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Statistics\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader)\n",
        "    epoch_acc = 100.0 * correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    \"\"\"Evaluate CNN baseline (vision-only).\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, _, labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader)\n",
        "    epoch_acc = 100.0 * correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(\"CNN TRAINING FUNCTIONS DEFINED\")\n",
        "print(\"-\" * 30)"
      ],
      "metadata": {
        "id": "figMvHEinqvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train CNN Baseline Model**"
      ],
      "metadata": {
        "id": "wz5T8JvhpObU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNNBaseline(\n",
        "    num_classes=2,\n",
        "    dropout=0.3,\n",
        "    fine_tune=True\n",
        ").to(device)\n",
        "\n",
        "# Training configuration\n",
        "NUM_EPOCHS = 10\n",
        "LEARNING_RATE = 0.0001\n",
        "PATIENCE = 3 # Stop if no improvements for 3 epochs\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    lr=LEARNING_RATE,\n",
        "    weight_decay=1e-4\n",
        ")\n",
        "\n",
        "# Track training history\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'train_acc': [],\n",
        "    'val_loss': [],\n",
        "    'val_acc': []\n",
        "}\n",
        "\n",
        "# Track best model by val_loss\n",
        "best_val_loss = float('inf')\n",
        "best_epoch = 0\n",
        "patience_counter = 0\n",
        "\n",
        "print(\"-\"*30)\n",
        "print(\"Training CNN Baseline Model\")\n",
        "print(\"-\"*30)\n",
        "print(f\"Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"Optimizer: Adam\")\n",
        "print(\"-\"*30)\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    print(f\"\\nEpoch {epoch}/{NUM_EPOCHS}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # Train\n",
        "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "\n",
        "    # Validate\n",
        "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
        "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "    # Save best model based on val_loss\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_epoch = epoch\n",
        "        patience_counter = 0\n",
        "\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_loss': val_loss,\n",
        "            'val_acc': val_acc,\n",
        "            'train_loss': train_loss,\n",
        "            'train_acc': train_acc,\n",
        "            'history': history,\n",
        "        }, f'{DATA_ROOT}/cnn_best_model.pth')\n",
        "\n",
        "        print(f\"Best model saved (Val Loss improved: {val_loss:.4f})\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(f\"No improvement (Patience: {patience_counter}/{PATIENCE})\")\n",
        "\n",
        "        # Early stopping\n",
        "        if patience_counter >= PATIENCE:\n",
        "            print(f\"\\n{'='*50}\")\n",
        "            print(f\"Early stopping triggered at epoch {epoch}\")\n",
        "            print(f\"Best model from epoch {best_epoch}\")\n",
        "            print(f\"Best val_loss: {best_val_loss:.4f}\")\n",
        "            print(f\"{'='*50}\")\n",
        "            break\n",
        "\n",
        "print(\"\\n\" + \"-\"*50)\n",
        "print(\"Training Completed!\")\n",
        "print(\"-\"*50)\n",
        "print(f\"Best epoch: {best_epoch}\")\n",
        "print(f\"Best val_loss: {best_val_loss:.4f}\")\n",
        "print(f\"Total epochs trained: {len(history['train_loss'])}\")\n",
        "print(\"-\"*50)"
      ],
      "metadata": {
        "id": "l3Yi8Xrso4Lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ploting the Curves**"
      ],
      "metadata": {
        "id": "NHRscfcQTEPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "os.makedirs(DATA_ROOT, exist_ok=True)\n",
        "\n",
        "epochs = np.arange(1, len(history['train_loss']) + 1)\n",
        "\n",
        "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Loss Curves\n",
        "axes[0].plot(\n",
        "    epochs, history['train_loss'],\n",
        "    marker='o', linewidth=2, label='Train Loss'\n",
        ")\n",
        "axes[0].plot(\n",
        "    epochs, history['val_loss'],\n",
        "    marker='s', linewidth=2, label='Validation Loss'\n",
        ")\n",
        "\n",
        "axes[0].axvline(\n",
        "    best_epoch, linestyle='--', linewidth=2,\n",
        "    label=f'Best Epoch ({best_epoch})'\n",
        ")\n",
        "\n",
        "axes[0].scatter(\n",
        "    best_epoch,\n",
        "    history['val_loss'][best_epoch - 1],\n",
        "    s=120, zorder=5\n",
        ")\n",
        "\n",
        "axes[0].set_title(\"Training vs Validation Loss\", fontsize=14, fontweight=\"bold\")\n",
        "axes[0].set_xlabel(\"Epoch\")\n",
        "axes[0].set_ylabel(\"Loss\")\n",
        "axes[0].legend()\n",
        "axes[0].set_ylim(0.3, 0.9)\n",
        "\n",
        "# Accuracy Curves\n",
        "axes[1].plot(\n",
        "    epochs, history['train_acc'],\n",
        "    marker='o', linewidth=2, label='Train Accuracy'\n",
        ")\n",
        "axes[1].plot(\n",
        "    epochs, history['val_acc'],\n",
        "    marker='s', linewidth=2, label='Validation Accuracy'\n",
        ")\n",
        "\n",
        "axes[1].axvline(\n",
        "    best_epoch, linestyle='--', linewidth=2,\n",
        "    label=f'Best Epoch ({best_epoch})'\n",
        ")\n",
        "\n",
        "axes[1].scatter(\n",
        "    best_epoch,\n",
        "    history['val_acc'][best_epoch - 1],\n",
        "    s=120, zorder=5\n",
        ")\n",
        "\n",
        "axes[1].set_title(\"Training vs Validation Accuracy\", fontsize=14, fontweight=\"bold\")\n",
        "axes[1].set_xlabel(\"Epoch\")\n",
        "axes[1].set_ylabel(\"Accuracy (%)\")\n",
        "axes[1].legend()\n",
        "axes[1].set_ylim(50, 85)\n",
        "\n",
        "plt.suptitle(\n",
        "    \"CNN Baseline Training Dynamics on VQA-RAD (Binary Yes/No)\",\n",
        "    fontsize=16, fontweight=\"bold\"\n",
        ")\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.savefig(f\"{DATA_ROOT}/cnn_training_curves.png\", dpi=300, bbox_inches=\"tight\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "D-BbomJNTDmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading the best model**"
      ],
      "metadata": {
        "id": "vXXlUh31UNAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load(\n",
        "    f'{DATA_ROOT}/cnn_best_model.pth',\n",
        "    map_location=device\n",
        ")\n",
        "\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "\n",
        "print(f\"Loaded trained model from epoch {checkpoint['epoch']}\")\n",
        "print(f\"Validation Accuracy at best epoch: {checkpoint['val_acc']:.2f}%\")"
      ],
      "metadata": {
        "id": "f7qMXNAIUPzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaulate on TEST set**"
      ],
      "metadata": {
        "id": "qpiY2fAes6UQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
        "\n",
        "print(\"-\"*50)\n",
        "print(\"TEST SET EVALUATION\")\n",
        "print(\"-\"*50)\n",
        "print(f\"Model: Best from Epoch {checkpoint['epoch']}\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
        "print(\"-\"*50)\n",
        "\n",
        "# Compare validation vs test\n",
        "print(f\"\\nValidation vs Test Comparison:\")\n",
        "print(\"-\"*50)\n",
        "print(f\"  Validation Acc: {checkpoint['val_acc']:.2f}%\")\n",
        "print(f\"  Test Acc:       {test_acc:.2f}%\")\n",
        "print(f\"  Difference:     {abs(test_acc - checkpoint['val_acc']):.2f} percentage points\")\n",
        "\n",
        "if abs(test_acc - checkpoint['val_acc']) < 5:\n",
        "    print(\"\\nTest accuracy is close to validation - good generalization!\")\n",
        "    print(\"  Model is properly trained and should work on new medical images.\")\n",
        "else:\n",
        "    print(\"\\nLarger gap between val and test - may indicate distribution differences\")\n",
        "\n",
        "print(\"-\"*50)\n",
        "\n",
        "# Get predictions for confusion matrix\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, _, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "print(\"\\nConfusion Matrix\")\n",
        "print(\"-\"*50)\n",
        "print(\"              Predicted\")\n",
        "print(\"              No    Yes\")\n",
        "print(f\"Actual No     {cm[0][0]:<5} {cm[0][1]:<5}\")\n",
        "print(f\"Actual Yes    {cm[1][0]:<5} {cm[1][1]:<5}\")\n",
        "print(\"-\"*50)\n",
        "\n",
        "# Calculate metrics from confusion matrix\n",
        "tn, fp, fn, tp = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
        "print(f\"\\nConfusion Matrix Breakdown:\")\n",
        "print(f\"  True Negatives (TN):  {tn} (Correctly predicted 'No')\")\n",
        "print(f\"  False Positives (FP): {fp} (Incorrectly predicted 'Yes')\")\n",
        "print(f\"  False Negatives (FN): {fn} (Incorrectly predicted 'No')\")\n",
        "print(f\"  True Positives (TP):  {tp} (Correctly predicted 'Yes')\")\n",
        "print(\"-\"*50)\n",
        "\n",
        "# Classification report\n",
        "report = classification_report(all_labels, all_preds, target_names=['No', 'Yes'], output_dict=True)\n",
        "print(\"\\nClassification Report\")\n",
        "print(\"-\"*50)\n",
        "print(f\"Class 'No':  Precision={report['No']['precision']:.2f}, \"\n",
        "      f\"Recall={report['No']['recall']:.2f}, F1={report['No']['f1-score']:.2f}\")\n",
        "print(f\"Class 'Yes': Precision={report['Yes']['precision']:.2f}, \"\n",
        "      f\"Recall={report['Yes']['recall']:.2f}, F1={report['Yes']['f1-score']:.2f}\")\n",
        "print(f\"\\nOverall Accuracy: {report['accuracy']:.4f} ({test_acc:.2f}%)\")\n",
        "print(f\"Support: No={int(report['No']['support'])}, Yes={int(report['Yes']['support'])}\")\n",
        "print(\"-\"*50)\n",
        "\n",
        "# Calculate recall balance (important for detecting bias)\n",
        "recall_no = report['No']['recall']\n",
        "recall_yes = report['Yes']['recall']\n",
        "recall_diff = abs(recall_no - recall_yes)\n",
        "\n",
        "print(f\"\\nRecall Balance Analysis:\")\n",
        "print(f\"  Recall (No):  {recall_no:.2f}\")\n",
        "print(f\"  Recall (Yes): {recall_yes:.2f}\")\n",
        "print(f\"  Difference:   {recall_diff:.2f}\")\n",
        "\n",
        "if recall_diff < 0.15:\n",
        "    print(\"  Balanced recall - model performs similarly on both classes\")\n",
        "else:\n",
        "    print(\"  Imbalanced recall - model favors one class over the other\")\n",
        "\n",
        "print(\"-\"*50)"
      ],
      "metadata": {
        "id": "LI1KMYfNpNW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Saving CNN Baseline Results**"
      ],
      "metadata": {
        "id": "6dmRy_hlul8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "os.makedirs(DATA_ROOT, exist_ok=True)\n",
        "\n",
        "CNN_RESULTS = {\n",
        "    'model_name': 'CNN Baseline (ResNet18, vision-only)',\n",
        "    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "\n",
        "    'architecture': {\n",
        "        'backbone': 'ResNet18 (ImageNet pretrained)',\n",
        "        'fine_tuning': 'Last residual block (layer4) + classifier',\n",
        "        'input': 'Image only',\n",
        "        'output_classes': 2\n",
        "    },\n",
        "\n",
        "    'training': {\n",
        "        'epochs_trained': len(history['train_loss']),\n",
        "        'batch_size': BATCH_SIZE,\n",
        "        'learning_rate': LEARNING_RATE,\n",
        "        'optimizer': 'Adam',\n",
        "        'best_epoch': best_epoch,\n",
        "        'best_val_acc': round(checkpoint['val_acc'], 2),\n",
        "        'training_history': history\n",
        "    },\n",
        "\n",
        "    'dataset': {\n",
        "        'train_samples': len(train),\n",
        "        'val_samples': len(val),\n",
        "        'test_samples': len(test)\n",
        "    },\n",
        "\n",
        "    'test_results': {\n",
        "        'accuracy': round(test_acc, 2),\n",
        "        'loss': round(test_loss, 4),\n",
        "        'confusion_matrix': {\n",
        "            'TN': int(cm[0][0]),\n",
        "            'FP': int(cm[0][1]),\n",
        "            'FN': int(cm[1][0]),\n",
        "            'TP': int(cm[1][1])\n",
        "        },\n",
        "        'classification_report': {\n",
        "            'no_precision': round(report['No']['precision'], 2),\n",
        "            'no_recall': round(report['No']['recall'], 2),\n",
        "            'no_f1': round(report['No']['f1-score'], 2),\n",
        "            'yes_precision': round(report['Yes']['precision'], 2),\n",
        "            'yes_recall': round(report['Yes']['recall'], 2),\n",
        "            'yes_f1': round(report['Yes']['f1-score'], 2)\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(f'{DATA_ROOT}/CNN_BASELINE_RESULTS.json', 'w') as f:\n",
        "    json.dump(CNN_RESULTS, f, indent=2)\n",
        "\n",
        "with open(f'{DATA_ROOT}/cnn_test_predictions.pkl', 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'predictions': all_preds,\n",
        "        'ground_truth': all_labels\n",
        "    }, f)\n",
        "\n",
        "print(\"-\"*30)\n",
        "print(\"CNN BASELINE RESULTS SAVED\")\n",
        "print(\"-\"*30)\n",
        "print(f\"Location: {DATA_ROOT}\")\n",
        "print(\"Files saved:\")\n",
        "print(\"  - CNN_BASELINE_RESULTS.json\")\n",
        "print(\"  - cnn_best_model.pth\")\n",
        "print(\"  - cnn_test_predictions.pkl\")\n",
        "print(\"-\"*30)"
      ],
      "metadata": {
        "id": "0oHgf5H9uiuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install BLIP-2 Dependencies**\n",
        "\n"
      ],
      "metadata": {
        "id": "QDKX5iwKTP0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.36.2 --break-system-packages --quiet\n",
        "!pip install accelerate --break-system-packages --quiet\n",
        "!pip install bitsandbytes --break-system-packages --quiet\n",
        "\n",
        "print(\"-\"*30)\n",
        "print(\"BLIP-2 Dependencies Installed\")\n",
        "print(\"-\"*30)"
      ],
      "metadata": {
        "id": "gDC013A4Tguu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
      ],
      "metadata": {
        "id": "L5EX-NCYTor1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes --quiet"
      ],
      "metadata": {
        "id": "7PnOAD_LXMqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading BLIP-2 Model**"
      ],
      "metadata": {
        "id": "P-s3a14NWp5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Blip2Processor, Blip2ForConditionalGeneration, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True\n",
        ")\n",
        "\n",
        "processor = Blip2Processor.from_pretrained(\n",
        "    \"Salesforce/blip2-opt-2.7b\",\n",
        "    use_fast=False\n",
        ")\n",
        "\n",
        "blip2_model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip2-opt-2.7b\",\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "blip2_model.eval()\n",
        "\n",
        "print(\"BLIP-2 loaded successfully\")\n",
        "print(\"Device:\", blip2_model.device)"
      ],
      "metadata": {
        "id": "YD0VU869U_d0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Reload full dataset\n",
        "vqa_data = pd.read_json(ANNOTATION_FILE)\n",
        "\n",
        "# Load saved split QIDs\n",
        "with open(f\"{DATA_ROOT}/split_qids.json\", \"r\") as f:\n",
        "    split_ids = json.load(f)\n",
        "\n",
        "# Reconstruct splits\n",
        "train = vqa_data[vqa_data['qid'].isin(split_ids['train_qids'])].reset_index(drop=True)\n",
        "val   = vqa_data[vqa_data['qid'].isin(split_ids['val_qids'])].reset_index(drop=True)\n",
        "test  = vqa_data[vqa_data['qid'].isin(split_ids['test_qids'])].reset_index(drop=True)\n",
        "\n",
        "print(\"Splits restored:\")\n",
        "print(f\"  Train: {len(train)}\")\n",
        "print(f\"  Val:   {len(val)}\")\n",
        "print(f\"  Test:  {len(test)}\")"
      ],
      "metadata": {
        "id": "56FTsPkNaF_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BLIP-2 VQA Prediction Function**"
      ],
      "metadata": {
        "id": "2keFrZk6YGLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def blip2_vqa_predict(image, question, model, processor):\n",
        "    \"\"\"\n",
        "    Zero-shot VQA using BLIP-2\n",
        "\n",
        "    Args:\n",
        "        image: PIL Image\n",
        "        question: str\n",
        "        model: BLIP-2 model\n",
        "        processor: BLIP-2 processor\n",
        "\n",
        "    Returns:\n",
        "        str: Predicted answer\n",
        "    \"\"\"\n",
        "    # Format prompt for VQA\n",
        "    prompt = f\"Question: {question} Answer:\"\n",
        "\n",
        "    # Preprocess inputs\n",
        "    inputs = processor(\n",
        "        images=image,\n",
        "        text=prompt,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device, torch.float16)\n",
        "\n",
        "    # Generate answer\n",
        "    generated_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=10,   # Short answers for yes/no\n",
        "        num_beams=1,         # Greedy decoding\n",
        "        do_sample=False      # Deterministic\n",
        "    )\n",
        "\n",
        "    # Decode answer\n",
        "    generated_text = processor.batch_decode(\n",
        "    generated_ids,\n",
        "    skip_special_tokens=True\n",
        "    )[0].strip().lower()\n",
        "\n",
        "    if \"yes\" in generated_text:\n",
        "      return \"yes\"\n",
        "    elif \"no\" in generated_text:\n",
        "      return \"no\"\n",
        "    else:\n",
        "      return \"unknown\"\n",
        "\n",
        "print(\"-\"*30)\n",
        "print(\"BLIP-2 VQA Function Defined\")\n",
        "print(\"-\"*30)\n",
        "\n",
        "# Test on one sample\n",
        "sample_row = test.iloc[0]\n",
        "sample_image_path = os.path.join(IMAGE_DIR, sample_row['image_name'])\n",
        "sample_image = Image.open(sample_image_path).convert('RGB')\n",
        "sample_question = sample_row['question']\n",
        "sample_answer = sample_row['answer']\n",
        "\n",
        "test_prediction = blip2_vqa_predict(sample_image, sample_question, blip2_model, processor)\n",
        "\n",
        "print(f\"\\nTest prediction:\")\n",
        "print(f\"  Question: {sample_question}\")\n",
        "print(f\"  Ground Truth: {sample_answer}\")\n",
        "print(f\"  BLIP-2 Prediction: {test_prediction}\")\n",
        "print(\"-\"*30)"
      ],
      "metadata": {
        "id": "Ny0YctlWWs33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate BLIP-2 Zero-Shot on Test Set**"
      ],
      "metadata": {
        "id": "YmN1YleXYqvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"-\"*30)\n",
        "print(\"Evaluating BLIP-2 ZERO-SHOT on Test Set\")\n",
        "print(\"-\"*30)\n",
        "\n",
        "# Filter binary test set (same as CNN)\n",
        "test_binary = test[test['answer'].str.lower().isin(['yes', 'no'])].reset_index(drop=True)\n",
        "print(f\"Binary test samples: {len(test_binary)}\")\n",
        "print(\"-\"*30)\n",
        "\n",
        "blip2_predictions = []\n",
        "blip2_ground_truth = []\n",
        "blip2_raw_outputs = []\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "for idx, row in tqdm(test_binary.iterrows(), total=len(test_binary), desc=\"Evaluating BLIP-2\"):\n",
        "    # Load image\n",
        "    image_path = os.path.join(IMAGE_DIR, row['image_name'])\n",
        "    try:\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "    except:\n",
        "        print(f\"Warning: Could not load {row['image_name']}\")\n",
        "        continue\n",
        "\n",
        "    question = row['question']\n",
        "    ground_truth = row['answer'].lower().strip()\n",
        "\n",
        "    # BLIP-2 prediction (already normalized to yes/no/unknown)\n",
        "    prediction = blip2_vqa_predict(image, question, blip2_model, processor)\n",
        "    prediction_lower = prediction.lower().strip()\n",
        "\n",
        "    # Store\n",
        "    blip2_predictions.append(prediction_lower)\n",
        "    blip2_ground_truth.append(ground_truth)\n",
        "    blip2_raw_outputs.append(prediction)\n",
        "\n",
        "    # Correctness check\n",
        "    is_correct = False\n",
        "    if ground_truth == 'yes':\n",
        "        is_correct = prediction_lower == 'yes'\n",
        "    elif ground_truth == 'no':\n",
        "        is_correct = prediction_lower == 'no'\n",
        "\n",
        "    if is_correct:\n",
        "        correct += 1\n",
        "\n",
        "    total += 1\n",
        "\n",
        "# Accuracy\n",
        "blip2_accuracy = (correct / total) * 100\n",
        "\n",
        "print(\"-\"*30)\n",
        "print(\"BLIP-2 ZERO-SHOT Evaluation Complete\")\n",
        "print(\"-\"*30)\n",
        "print(f\"Total samples: {total}\")\n",
        "print(f\"Correct predictions: {correct}\")\n",
        "print(f\"Accuracy: {blip2_accuracy:.2f}%\")\n",
        "print(\"-\"*30)"
      ],
      "metadata": {
        "id": "TtwDQQv3YZQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Caclculate BLIP-2 Detailed Metrics**"
      ],
      "metadata": {
        "id": "dDtIdX1waW9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "print(\"-\"*30)\n",
        "print(\"Calculating BLIP-2 Metrics\")\n",
        "print(\"-\"*30)\n",
        "\n",
        "# Convert to binary labels (0=no, 1=yes)\n",
        "blip2_pred_binary = []\n",
        "blip2_true_binary = []\n",
        "\n",
        "for pred, true in zip(blip2_predictions, blip2_ground_truth):\n",
        "    # Ground truth\n",
        "    true_label = 1 if true == 'yes' else 0\n",
        "    blip2_true_binary.append(true_label)\n",
        "\n",
        "    # Prediction (flexible matching)\n",
        "    if 'yes' in pred and 'no' not in pred:\n",
        "        pred_label = 1\n",
        "    elif 'no' in pred:\n",
        "        pred_label = 0\n",
        "    else:\n",
        "        pred_label = 0  # Default to 'no' if unclear\n",
        "\n",
        "    blip2_pred_binary.append(pred_label)\n",
        "\n",
        "# Confusion matrix\n",
        "cm_blip2 = confusion_matrix(blip2_true_binary, blip2_pred_binary)\n",
        "\n",
        "print(\"\\nConfusion Matrix - BLIP-2 ZERO-SHOT\")\n",
        "print(\"-\"*30)\n",
        "print(\"              Predicted\")\n",
        "print(\"              No    Yes\")\n",
        "print(f\"Actual No     {cm_blip2[0][0]:<5} {cm_blip2[0][1]:<5}\")\n",
        "print(f\"Actual Yes    {cm_blip2[1][0]:<5} {cm_blip2[1][1]:<5}\")\n",
        "print(\"-\"*30)\n",
        "\n",
        "# Classification report\n",
        "report_blip2 = classification_report(\n",
        "    blip2_true_binary,\n",
        "    blip2_pred_binary,\n",
        "    target_names=['No', 'Yes'],\n",
        "    output_dict=True\n",
        ")\n",
        "\n",
        "print(\"\\nClassification Report\")\n",
        "print(\"-\"*30)\n",
        "print(f\"Class 'No':  Precision={report_blip2['No']['precision']:.2f}, \"\n",
        "      f\"Recall={report_blip2['No']['recall']:.2f}, F1={report_blip2['No']['f1-score']:.2f}\")\n",
        "print(f\"Class 'Yes': Precision={report_blip2['Yes']['precision']:.2f}, \"\n",
        "      f\"Recall={report_blip2['Yes']['recall']:.2f}, F1={report_blip2['Yes']['f1-score']:.2f}\")\n",
        "print(f\"\\nAccuracy: {report_blip2['accuracy']:.2f}\")\n",
        "print(f\"Support No: {int(report_blip2['No']['support'])}, Support Yes: {int(report_blip2['Yes']['support'])}\")\n",
        "print(\"-\"*30)"
      ],
      "metadata": {
        "id": "xOGCFZe-ZDiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save BLIP-2 Results & Comparison**"
      ],
      "metadata": {
        "id": "3G829as1bE8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import pickle\n",
        "\n",
        "blip2_accuracy_corrected = report_blip2['accuracy'] * 100\n",
        "\n",
        "# Save BLIP-2 results\n",
        "BLIP2_RESULTS = {\n",
        "    'model_name': 'BLIP-2 Zero-Shot (OPT-2.7B)',\n",
        "    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "\n",
        "    'architecture': {\n",
        "        'vision_encoder': 'EVA-CLIP (ViT-g/14)',\n",
        "        'language_model': 'OPT-2.7B',\n",
        "        'total_params': '2.7B',\n",
        "        'approach': 'zero-shot (no training)'\n",
        "    },\n",
        "\n",
        "    'dataset': {\n",
        "        'test_samples': total\n",
        "    },\n",
        "\n",
        "    'test_results': {\n",
        "        'accuracy': round(blip2_accuracy_corrected, 2),\n",
        "        'confusion_matrix': {\n",
        "            'true_no_pred_no': int(cm_blip2[0][0]),\n",
        "            'true_no_pred_yes': int(cm_blip2[0][1]),\n",
        "            'true_yes_pred_no': int(cm_blip2[1][0]),\n",
        "            'true_yes_pred_yes': int(cm_blip2[1][1])\n",
        "        },\n",
        "        'classification_report': {\n",
        "            'no_precision': round(report_blip2['No']['precision'], 2),\n",
        "            'no_recall': round(report_blip2['No']['recall'], 2),\n",
        "            'no_f1': round(report_blip2['No']['f1-score'], 2),\n",
        "            'yes_precision': round(report_blip2['Yes']['precision'], 2),\n",
        "            'yes_recall': round(report_blip2['Yes']['recall'], 2),\n",
        "            'yes_f1': round(report_blip2['Yes']['f1-score'], 2),\n",
        "            'support_no': int(report_blip2['No']['support']),\n",
        "            'support_yes': int(report_blip2['Yes']['support'])\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save BLIP-2 results\n",
        "with open(f'{DATA_ROOT}/BLIP2_BASELINE_RESULTS.json', 'w') as f:\n",
        "    json.dump(BLIP2_RESULTS, f, indent=2)\n",
        "\n",
        "with open(f'{DATA_ROOT}/blip2_test_predictions.pkl', 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'predictions': blip2_predictions,\n",
        "        'predictions_binary': blip2_pred_binary,\n",
        "        'ground_truth': blip2_ground_truth,\n",
        "        'ground_truth_binary': blip2_true_binary,\n",
        "        'raw_outputs': blip2_raw_outputs\n",
        "    }, f)\n",
        "\n",
        "# Load CNN results for comparison\n",
        "with open(f'{DATA_ROOT}/CNN_BASELINE_RESULTS.json', 'r') as f:\n",
        "    cnn_results = json.load(f)\n",
        "\n",
        "print(\"-\"*70)\n",
        "print(\"FINAL COMPARISON: CNN vs BLIP-2\")\n",
        "print(\"-\"*70)\n",
        "print(f\"{'Metric':<30} {'CNN Baseline':<20} {'BLIP-2 Zero-Shot':<20}\")\n",
        "print(\"-\"*70)\n",
        "print(f\"{'Approach':<30} {'Trained (835 samples)':<20} {'Zero-shot':<20}\")\n",
        "print(f\"{'Parameters':<30} {'15.2M total':<20} {'2.7B frozen':<20}\")\n",
        "print(\"-\"*70)\n",
        "print(f\"{'Test Accuracy':<30} {cnn_results['test_results']['accuracy']:<20.2f} {blip2_accuracy_corrected:<20.2f}\")\n",
        "print(f\"{'Difference':<30} {'-':<20} {blip2_accuracy_corrected - cnn_results['test_results']['accuracy']:+.2f}\")\n",
        "print(\"-\"*70)\n",
        "print(f\"{'Precision (No)':<30} {cnn_results['test_results']['classification_report']['no_precision']:<20.2f} {report_blip2['No']['precision']:<20.2f}\")\n",
        "print(f\"{'Precision (Yes)':<30} {cnn_results['test_results']['classification_report']['yes_precision']:<20.2f} {report_blip2['Yes']['precision']:<20.2f}\")\n",
        "print(\"-\"*70)\n",
        "print(f\"{'Recall (No)':<30} {cnn_results['test_results']['classification_report']['no_recall']:<20.2f} {report_blip2['No']['recall']:<20.2f}\")\n",
        "print(f\"{'Recall (Yes)':<30} {cnn_results['test_results']['classification_report']['yes_recall']:<20.2f} {report_blip2['Yes']['recall']:<20.2f}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "# Save comparison\n",
        "comparison = {\n",
        "    'cnn_baseline': cnn_results['test_results'],\n",
        "    'blip2_zero_shot': BLIP2_RESULTS['test_results'],\n",
        "    'key_findings': {\n",
        "        'accuracy_difference': round(blip2_accuracy_corrected - cnn_results['test_results']['accuracy'], 2),\n",
        "        'winner': 'CNN' if cnn_results['test_results']['accuracy'] > blip2_accuracy_corrected else 'BLIP-2',\n",
        "        'cnn_bias': 'Balanced predictions',\n",
        "        'blip2_bias': 'Biased toward Yes (81% recall)'\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(f'{DATA_ROOT}/BASELINE_COMPARISON.json', 'w') as f:\n",
        "    json.dump(comparison, f, indent=2)\n",
        "\n",
        "print(\"\\nALL RESULTS SAVED:\")\n",
        "print(f\"  {DATA_ROOT}BLIP2_BASELINE_RESULTS.json\")\n",
        "print(f\"  {DATA_ROOT}BASELINE_COMPARISON.json\")\n",
        "print(\"-\"*30)"
      ],
      "metadata": {
        "id": "75xXybosauuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualizations**"
      ],
      "metadata": {
        "id": "8vj-toxj6qOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "print(\"-\"*30)\n",
        "print(\"Loading Data for Visualization\")\n",
        "print(\"-\"*30)\n",
        "\n",
        "\n",
        "\n",
        "# Load CNN results (includes training history)\n",
        "with open(f'{DATA_ROOT}/CNN_BASELINE_RESULTS.json', 'r') as f:\n",
        "    cnn_results = json.load(f)\n",
        "\n",
        "# Extract training history from saved results\n",
        "history = cnn_results['training']['training_history']  #  LOAD REAL HISTORY\n",
        "NUM_EPOCHS = cnn_results['training']['epochs_trained']\n",
        "\n",
        "# Load BLIP-2 results\n",
        "with open(f'{DATA_ROOT}/BLIP2_BASELINE_RESULTS.json', 'r') as f:\n",
        "    blip2_results = json.load(f)\n",
        "\n",
        "# Load CNN predictions\n",
        "with open(f'{DATA_ROOT}/cnn_test_predictions.pkl', 'rb') as f:\n",
        "    cnn_preds = pickle.load(f)\n",
        "\n",
        "# Load BLIP-2 predictions\n",
        "with open(f'{DATA_ROOT}/blip2_test_predictions.pkl', 'rb') as f:\n",
        "    blip2_preds = pickle.load(f)\n",
        "\n",
        "# Recreate confusion matrices\n",
        "cm_cnn = confusion_matrix(cnn_preds['ground_truth'], cnn_preds['predictions'])\n",
        "cm_blip2 = confusion_matrix(blip2_preds['ground_truth_binary'], blip2_preds['predictions_binary'])\n",
        "\n",
        "# Recreate classification reports\n",
        "report_cnn = classification_report(\n",
        "    cnn_preds['ground_truth'],\n",
        "    cnn_preds['predictions'],\n",
        "    target_names=['No', 'Yes'],\n",
        "    output_dict=True\n",
        ")\n",
        "\n",
        "report_blip2 = classification_report(\n",
        "    blip2_preds['ground_truth_binary'],\n",
        "    blip2_preds['predictions_binary'],\n",
        "    target_names=['No', 'Yes'],\n",
        "    output_dict=True\n",
        ")\n",
        "\n",
        "# Get key values\n",
        "blip2_accuracy_corrected = blip2_results['test_results']['accuracy']\n",
        "\n",
        "print(\"CNN results loaded\")\n",
        "print(\"BLIP-2 results loaded\")\n",
        "print(\"Training history loaded (REAL DATA)\")\n",
        "print(\"Predictions loaded\")\n",
        "print(\"Confusion matrices reconstructed\")\n",
        "print(\"Classification reports reconstructed\")\n",
        "print(\"-\"*30)\n",
        "print(f\"Training history epochs: {NUM_EPOCHS}\")\n",
        "print(f\"  Train loss: {history['train_loss']}\")\n",
        "print(f\"  Val loss: {history['val_loss']}\")\n",
        "print(f\"  Train acc: {history['train_acc']}\")\n",
        "print(f\"  Val acc: {history['val_acc']}\")\n",
        "print(\"-\"*30)\n",
        "print(\"Ready to generate ALL visualizations!\")\n",
        "print(\"-\"*30)"
      ],
      "metadata": {
        "id": "IuOZqXcZ8F0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "# Create output directory for images\n",
        "IMAGES_DIR = f'{DATA_ROOT}/presentation_images/'\n",
        "os.makedirs(IMAGES_DIR, exist_ok=True)\n",
        "\n",
        "print(\"-\"*30)\n",
        "print(\"Generating All Visualizations\")\n",
        "print(\"-\"*30)\n",
        "\n",
        "# Training curves (CNN)\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Loss curve\n",
        "epochs = range(1, NUM_EPOCHS + 1)\n",
        "ax1.plot(epochs, history['train_loss'], 'b-o', label='Train Loss', linewidth=2, markersize=8)\n",
        "ax1.plot(epochs, history['val_loss'], 'r-s', label='Val Loss', linewidth=2, markersize=8)\n",
        "ax1.set_xlabel('Epoch', fontsize=14)\n",
        "ax1.set_ylabel('Loss', fontsize=14)\n",
        "ax1.set_title('CNN Training and Validation Loss', fontsize=16, fontweight='bold')\n",
        "ax1.legend(fontsize=12)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.set_xticks(epochs)\n",
        "\n",
        "# Accuracy curve\n",
        "ax2.plot(epochs, history['train_acc'], 'b-o', label='Train Acc', linewidth=2, markersize=8)\n",
        "ax2.plot(epochs, history['val_acc'], 'r-s', label='Val Acc', linewidth=2, markersize=8)\n",
        "ax2.set_xlabel('Epoch', fontsize=14)\n",
        "ax2.set_ylabel('Accuracy (%)', fontsize=14)\n",
        "ax2.set_title('CNN Training and Validation Accuracy', fontsize=16, fontweight='bold')\n",
        "ax2.legend(fontsize=12)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.set_xticks(epochs)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{IMAGES_DIR}01_cnn_training_curves.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"Training curves saved\")\n",
        "\n",
        "# Confusion Matrices (Side by Side)\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# CNN confusion matrix\n",
        "sns.heatmap(cm_cnn, annot=True, fmt='d', cmap='Blues', ax=ax1,\n",
        "            xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'],\n",
        "            cbar_kws={'label': 'Count'}, annot_kws={'size': 16})\n",
        "ax1.set_xlabel('Predicted', fontsize=14)\n",
        "ax1.set_ylabel('Actual', fontsize=14)\n",
        "ax1.set_title(f\"CNN Baseline ({cnn_results['test_results']['accuracy']:.2f}%)\", fontsize=16, fontweight='bold')\n",
        "\n",
        "# BLIP-2 confusion matrix\n",
        "sns.heatmap(cm_blip2, annot=True, fmt='d', cmap='Oranges', ax=ax2,\n",
        "            xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'],\n",
        "            cbar_kws={'label': 'Count'}, annot_kws={'size': 16})\n",
        "ax2.set_xlabel('Predicted', fontsize=14)\n",
        "ax2.set_ylabel('Actual', fontsize=14)\n",
        "ax2.set_title(f'BLIP-2 Zero-Shot ({blip2_accuracy_corrected:.2f}%)', fontsize=16, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{IMAGES_DIR}02_confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"Confusion matrices saved\")\n",
        "\n",
        "# Accuracy Comparison Bar Chart\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "models = ['CNN Baseline\\n(Trained)', 'BLIP-2\\n(Zero-Shot)']\n",
        "accuracies = [cnn_results['test_results']['accuracy'], blip2_accuracy_corrected]\n",
        "colors = ['#2E86AB', '#F77F00']\n",
        "\n",
        "bars = ax.bar(models, accuracies, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
        "\n",
        "# Add value labels on bars\n",
        "for i, (bar, acc) in enumerate(zip(bars, accuracies)):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
        "            f'{acc:.2f}%', ha='center', va='bottom', fontsize=16, fontweight='bold')\n",
        "\n",
        "ax.set_ylabel('Test Accuracy (%)', fontsize=14)\n",
        "ax.set_title('Model Performance Comparison', fontsize=18, fontweight='bold')\n",
        "ax.set_ylim(0, 100)\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "ax.axhline(y=50, color='red', linestyle='--', linewidth=2, alpha=0.5, label='Random Baseline')\n",
        "ax.legend(fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{IMAGES_DIR}03_accuracy_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"Accuracy comparison saved\")\n",
        "\n",
        "# Precision, Recall, F1 Comparison\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Metrics for \"No\" class\n",
        "metrics_no = ['Precision', 'Recall', 'F1-Score']\n",
        "cnn_no = [\n",
        "    cnn_results['test_results']['classification_report']['no_precision'],\n",
        "    cnn_results['test_results']['classification_report']['no_recall'],\n",
        "    cnn_results['test_results']['classification_report']['no_f1']\n",
        "]\n",
        "blip2_no = [\n",
        "    report_blip2['No']['precision'],\n",
        "    report_blip2['No']['recall'],\n",
        "    report_blip2['No']['f1-score']\n",
        "]\n",
        "\n",
        "x = np.arange(len(metrics_no))\n",
        "width = 0.35\n",
        "\n",
        "ax1.bar(x - width/2, cnn_no, width, label='CNN', color='#2E86AB', alpha=0.8, edgecolor='black')\n",
        "ax1.bar(x + width/2, blip2_no, width, label='BLIP-2', color='#F77F00', alpha=0.8, edgecolor='black')\n",
        "ax1.set_xlabel('Metric', fontsize=14)\n",
        "ax1.set_ylabel('Score', fontsize=14)\n",
        "ax1.set_title('Class \"No\" Performance', fontsize=16, fontweight='bold')\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(metrics_no)\n",
        "ax1.set_ylim(0, 1.0)\n",
        "ax1.legend(fontsize=12)\n",
        "ax1.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Metrics for \"Yes\" class\n",
        "cnn_yes = [\n",
        "    cnn_results['test_results']['classification_report']['yes_precision'],\n",
        "    cnn_results['test_results']['classification_report']['yes_recall'],\n",
        "    cnn_results['test_results']['classification_report']['yes_f1']\n",
        "]\n",
        "blip2_yes = [\n",
        "    report_blip2['Yes']['precision'],\n",
        "    report_blip2['Yes']['recall'],\n",
        "    report_blip2['Yes']['f1-score']\n",
        "]\n",
        "\n",
        "ax2.bar(x - width/2, cnn_yes, width, label='CNN', color='#2E86AB', alpha=0.8, edgecolor='black')\n",
        "ax2.bar(x + width/2, blip2_yes, width, label='BLIP-2', color='#F77F00', alpha=0.8, edgecolor='black')\n",
        "ax2.set_xlabel('Metric', fontsize=14)\n",
        "ax2.set_ylabel('Score', fontsize=14)\n",
        "ax2.set_title('Class \"Yes\" Performance', fontsize=16, fontweight='bold')\n",
        "ax2.set_xticks(x)\n",
        "ax2.set_xticklabels(metrics_no)\n",
        "ax2.set_ylim(0, 1.0)\n",
        "ax2.legend(fontsize=12)\n",
        "ax2.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{IMAGES_DIR}04_metrics_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"Metrics comparison saved\")\n",
        "\n",
        "# Test Set Class Distribution Pie Chart\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "\n",
        "sizes = [\n",
        "    sum(np.array(cnn_preds['ground_truth']) == 0),  # No\n",
        "    sum(np.array(cnn_preds['ground_truth']) == 1)   # Yes\n",
        "]\n",
        "labels = ['No', 'Yes']\n",
        "colors = ['#FF6B6B', '#4ECDC4']\n",
        "explode = (0.05, 0.05)\n",
        "\n",
        "wedges, texts, autotexts = ax.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
        "                                    autopct='%1.1f%%', shadow=True, startangle=90,\n",
        "                                    textprops={'fontsize': 14, 'fontweight': 'bold'})\n",
        "\n",
        "# Add count labels\n",
        "for i, (label, size) in enumerate(zip(labels, sizes)):\n",
        "    texts[i].set_text(f'{label}\\n({size} samples)')\n",
        "\n",
        "ax.set_title('Test Set Class Distribution (179 samples)', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{IMAGES_DIR}05_class_distribution.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"Class distribution saved\")\n",
        "\n",
        "# Model Architecture Comparison\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "categories = ['Parameters\\n(Millions)', 'Training\\nSamples', 'Training\\nTime (min)']\n",
        "cnn_vals = [15.2, 835, 5]\n",
        "ax.text(1 - width/2, 95, '835', ha='center', fontsize=11, fontweight='bold')\n",
        "blip2_vals = [2700, 0, 0]  # 2700M params, 0 samples\n",
        "\n",
        "x = np.arange(len(categories))\n",
        "width = 0.35\n",
        "\n",
        "# Scale for visualization\n",
        "cnn_display = [15.2, 90.8, 50]  # Scaled\n",
        "blip2_display = [270, 0, 0]  # Scaled\n",
        "\n",
        "bars1 = ax.bar(x - width/2, cnn_display, width, label='CNN Baseline',\n",
        "               color='#2E86AB', alpha=0.8, edgecolor='black')\n",
        "bars2 = ax.bar(x + width/2, blip2_display, width, label='BLIP-2 Zero-Shot',\n",
        "               color='#F77F00', alpha=0.8, edgecolor='black')\n",
        "\n",
        "ax.set_ylabel('Relative Scale', fontsize=14)\n",
        "ax.set_title('Model Comparison: Resources & Training', fontsize=16, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(categories, fontsize=12)\n",
        "ax.legend(fontsize=12)\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add actual values as text\n",
        "ax.text(0 - width/2, 20, '15.2M', ha='center', fontsize=11, fontweight='bold')\n",
        "ax.text(0 + width/2, 280, '2.7B', ha='center', fontsize=11, fontweight='bold')\n",
        "ax.text(1 - width/2, 95, '835', ha='center', fontsize=11, fontweight='bold')\n",
        "ax.text(1 + width/2, 5, 'None', ha='center', fontsize=11, fontweight='bold')\n",
        "ax.text(2 - width/2, 55, '~5 min', ha='center', fontsize=11, fontweight='bold')\n",
        "ax.text(2 + width/2, 5, 'None', ha='center', fontsize=11, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{IMAGES_DIR}06_model_resources.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"Model resources comparison saved\")\n",
        "\n",
        "# Recall Patterns (Highlighting Bias)\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "classes = ['No', 'Yes']\n",
        "cnn_recall = [\n",
        "    cnn_results['test_results']['classification_report']['no_recall'],\n",
        "    cnn_results['test_results']['classification_report']['yes_recall']\n",
        "]\n",
        "blip2_recall = [\n",
        "    report_blip2['No']['recall'],\n",
        "    report_blip2['Yes']['recall']\n",
        "]\n",
        "\n",
        "x = np.arange(len(classes))\n",
        "width = 0.35\n",
        "\n",
        "bars1 = ax.bar(x - width/2, cnn_recall, width, label='CNN (Balanced)',\n",
        "               color='#2E86AB', alpha=0.8, edgecolor='black', linewidth=2)\n",
        "bars2 = ax.bar(x + width/2, blip2_recall, width, label='BLIP-2 (Biased to Yes)',\n",
        "               color='#F77F00', alpha=0.8, edgecolor='black', linewidth=2)\n",
        "\n",
        "# Add value labels\n",
        "for bars in [bars1, bars2]:\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
        "                f'{height:.2f}', ha='center', va='bottom', fontsize=13, fontweight='bold')\n",
        "\n",
        "ax.set_ylabel('Recall', fontsize=14)\n",
        "ax.set_title('Recall Comparison: Prediction Bias Analysis', fontsize=16, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(classes, fontsize=14)\n",
        "ax.set_ylim(0, 1.0)\n",
        "ax.axhline(y=0.7, color='green', linestyle='--', linewidth=2, alpha=0.5, label='Good Threshold')\n",
        "ax.legend(fontsize=12, loc='lower right')\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{IMAGES_DIR}07_recall_bias.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"Recall bias analysis saved\")\n",
        "\n",
        "print(\"-\"*30)\n",
        "print(\"All Visualizations\")\n",
        "print(\"-\"*30)\n",
        "print(f\"Location: {IMAGES_DIR}\")\n",
        "print(\"\\nGenerated images:\")\n",
        "print(\"  01_cnn_training_curves.png\")\n",
        "print(\"  02_confusion_matrices.png\")\n",
        "print(\"  03_accuracy_comparison.png\")\n",
        "print(\"  04_metrics_comparison.png\")\n",
        "print(\"  05_class_distribution.png\")\n",
        "print(\"  06_model_resources.png\")\n",
        "print(\"  07_recall_bias.png\")\n",
        "print(\"-\"*30)"
      ],
      "metadata": {
        "id": "z15-f-8ceVyz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}