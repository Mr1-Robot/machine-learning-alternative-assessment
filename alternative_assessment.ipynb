{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOpzRvhGvc8CvNE4DvGl+X+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mr1-Robot/machine-learning-alternative-assessment/blob/main/alternative_assessment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Alternative Assessment - WOA7015\n",
        "#####MUAAMAR MOHAMMED ABDULLAH AL-GHRAIRI - 24084470"
      ],
      "metadata": {
        "id": "d3YMC15UZMvQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8KlzI69XiO5"
      },
      "outputs": [],
      "source": [
        "# Mount google drive to access the dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Core libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Data processing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Utils\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import pickle\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"-\"*30)\n",
        "print(\"SETUP\")\n",
        "print(\"-\"*30)\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "  print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(\"-\"*30)\n"
      ],
      "metadata": {
        "id": "TF88Uj5tYtHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load & Explore **Dataset**"
      ],
      "metadata": {
        "id": "lDH11gZfdFB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths\n",
        "DATA_DIR = '/content/drive/MyDrive/alternative-assessment-materials/'\n",
        "JSON_PATH = f\"{DATA_DIR}VQA_RAD_Dataset.json\"\n",
        "IMAGE_DIR = f\"{DATA_DIR}VQA_RAD_Images\"\n",
        "\n",
        "print(\"-\"*30)\n",
        "print(\"Loading VQA-RAD Dataser\")\n",
        "print(\"-\"*30)\n",
        "print(f\"JSON file: {JSON_PATH}\")\n",
        "print(f\"Images folder: {IMAGE_DIR}\")\n",
        "print(\"-\"*30)\n",
        "\n",
        "# Load the JSON file into a pandas Dataframe\n",
        "vqa_data = pd.read_json(JSON_PATH)\n",
        "\n",
        "print(f\"\\nDataset loaded successfully!\")\n",
        "print(f\"Total Q&A pairs: {len(vqa_data)}\")\n",
        "print(\"\\nDataFrame columns:\")\n",
        "print(vqa_data.columns.tolist())\n",
        "print(\"\\nFirst 3 rows:\")\n",
        "print(vqa_data.head(3))\n",
        "\n",
        "print(\"-\"*30)\n",
        "print(\"DATASET STATISTICS\")\n",
        "print(\"-\"*30)\n",
        "\n",
        "# Answer types distribution\n",
        "answer_types = vqa_data['answer_type'].value_counts()\n",
        "print(\"\\nAnswer Type Distribution:\")\n",
        "for answer_type, count in answer_types.items():\n",
        "    percentage = (count / len(vqa_data)) * 100\n",
        "    print(f\"  {answer_type}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "# For closed-ended questions (yes/no)\n",
        "closed_data = vqa_data[vqa_data['answer_type'] == 'CLOSED']\n",
        "print(f\"\\nClosed-ended questions: {len(closed_data)}\")\n",
        "\n",
        "# Yes/No distribution\n",
        "answer_dist = closed_data['answer'].value_counts()\n",
        "print(\"\\nYes/No Distribution:\")\n",
        "for answer, count in answer_dist.items():\n",
        "    percentage = (count / len(closed_data)) * 100\n",
        "    print(f\"  {answer}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "# Organ distribution\n",
        "if 'phrase_type' in vqa_data.columns:\n",
        "    organ_dist = vqa_data['phrase_type'].value_counts()\n",
        "    print(\"\\nOrgan Distribution:\")\n",
        "    for organ, count in organ_dist.items():\n",
        "        percentage = (count / len(vqa_data)) * 100\n",
        "        print(f\"  {organ}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "print(\"-\"*30)"
      ],
      "metadata": {
        "id": "g8DtBq2-bgik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Create TRAIN/VAL/TEST splits"
      ],
      "metadata": {
        "id": "7yq4Gru-f7ll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Filter closed-ended questions only\n",
        "closed_data = vqa_data[vqa_data['answer_type'] == 'CLOSED'].copy()\n",
        "\n",
        "# Convery yes/no to binary labels (0=no, 1=yes)\n",
        "closed_data['label'] = closed_data['answer'].apply(lambda x: 1 if x.lower() == 'yes' else 0)\n",
        "\n",
        "print(\"-\"*30)\n",
        "print(\"Creating TEST/VAL/TEST splits\")\n",
        "print(\"-\"*30)\n",
        "print(f\"Total closed-ended samples: {len(closed_data)}\")\n",
        "print(f\"Yes: {closed_data['label'].sum()}, No: {len(closed_data) - closed_data['label'].sum()}\")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Split: 70% train, 15% val, 15% test (stratified by label)\n",
        "train_val, test = train_test_split(\n",
        "    closed_data,\n",
        "    test_size=0.15,          # 15% for test\n",
        "    random_state=42,\n",
        "    stratify=closed_data['label']  # Maintain yes/no ratio\n",
        ")\n",
        "\n",
        "train, val = train_test_split(\n",
        "    train_val,\n",
        "    test_size=0.176,         # 15% of total (0.15/0.85 â‰ˆ 0.176)\n",
        "    random_state=42,\n",
        "    stratify=train_val['label']\n",
        ")\n",
        "\n",
        "# Reset indices\n",
        "train = train.reset_index(drop=True)\n",
        "val = val.reset_index(drop=True)\n",
        "test = test.reset_index(drop=True)\n",
        "\n",
        "print(f\"\\nSplit sizes:\")\n",
        "print(f\"  Train: {len(train)} samples ({len(train)/len(closed_data)*100:.1f}%)\")\n",
        "print(f\"  Val:   {len(val)} samples ({len(val)/len(closed_data)*100:.1f}%)\")\n",
        "print(f\"  Test:  {len(test)} samples ({len(test)/len(closed_data)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nClass distribution:\")\n",
        "print(f\"  Train - Yes: {train['label'].sum()}, No: {len(train)-train['label'].sum()}\")\n",
        "print(f\"  Val   - Yes: {val['label'].sum()}, No: {len(val)-val['label'].sum()}\")\n",
        "print(f\"  Test  - Yes: {test['label'].sum()}, No: {len(test)-test['label'].sum()}\")\n",
        "print(\"-\"*30)\n",
        "\n",
        "# Save split indices for reproducibility\n",
        "SAVE_DIR = '/content/drive/MyDrive/alternative-assessment-materials/VQA_RAD_LOCKED_RESULTS/'\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "split_indices = {\n",
        "    'train_indices': train.index.tolist(),\n",
        "    'val_indices': val.index.tolist(),\n",
        "    'test_indices': test.index.tolist()\n",
        "}\n",
        "\n",
        "with open(f'{SAVE_DIR}/split_indices.json', 'w') as f:\n",
        "    json.dump(split_indices, f)\n",
        "\n",
        "print(f\"Split indices saved to: {SAVE_DIR}split_indices.json\")"
      ],
      "metadata": {
        "id": "yyt08zipd5Ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build **Vocabulary** from questions"
      ],
      "metadata": {
        "id": "LBZRzb19iAAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "print(\"-\"*30)\n",
        "print(\"Building Vocabulary\")\n",
        "print(\"-\"*30)\n",
        "\n",
        "# Tokenize all training questions\n",
        "all_words = []\n",
        "for question in train['question']:\n",
        "    # Simple tokenization: lowercase and split by spaces\n",
        "    words = question.lower().split()\n",
        "    all_words.extend(words)\n",
        "\n",
        "# Count word frequencies\n",
        "word_counts = Counter(all_words)\n",
        "print(f\"Total words in training set: {len(all_words)}\")\n",
        "print(f\"Unique words: {len(word_counts)}\")\n",
        "\n",
        "# Create vocabulary: word -> index mapping\n",
        "# Reserve indices: 0=PAD, 1=UNK (unknown words)\n",
        "vocab = {'<PAD>': 0, '<UNK>': 1}\n",
        "for word, count in word_counts.most_common():\n",
        "    if count >= 2:  # Only include words that appear at least twice\n",
        "        vocab[word] = len(vocab)\n",
        "\n",
        "print(f\"Vocabulary size (including PAD and UNK): {len(vocab)}\")\n",
        "print(f\"\\nMost common words:\")\n",
        "for word, count in word_counts.most_common(20):\n",
        "    print(f\"  '{word}': {count} times\")\n",
        "\n",
        "# Create reverse mapping: index -> word\n",
        "idx_to_word = {idx: word for word, idx in vocab.items()}\n",
        "\n",
        "print(\"-\"*30)\n",
        "\n",
        "# Test encoding a sample question\n",
        "sample_question = train.iloc[0]['question']\n",
        "sample_encoded = [vocab.get(word.lower(), vocab['<UNK>']) for word in sample_question.split()]\n",
        "\n",
        "print(f\"\\nSample encoding:\")\n",
        "print(f\"  Question: {sample_question}\")\n",
        "print(f\"  Encoded: {sample_encoded}\")\n",
        "print(f\"  Decoded: {' '.join([idx_to_word[idx] for idx in sample_encoded])}\")\n",
        "print(\"-\"*30)"
      ],
      "metadata": {
        "id": "fHn0-tZ3hv4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Create PyTorch Dataset**"
      ],
      "metadata": {
        "id": "MSyi-Qehitho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "class VQADataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset for VQA-RAD\"\"\"\n",
        "\n",
        "    def __init__(self, dataframe, vocab, image_dir, max_seq_length=20, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dataframe: DataFrame with columns [image_name, question, label]\n",
        "            vocab: Dictionary mapping words to indices\n",
        "            image_dir: Path to image folder\n",
        "            max_seq_length: Maximum question length (pad/truncate)\n",
        "            transform: Image transformations\n",
        "        \"\"\"\n",
        "        self.data = dataframe\n",
        "        self.vocab = vocab\n",
        "        self.image_dir = image_dir\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get data for this index\n",
        "        row = self.data.iloc[idx]\n",
        "        image_name = row['image_name']\n",
        "        question = row['question']\n",
        "        label = row['label']\n",
        "\n",
        "        # Load and transform image\n",
        "        image_path = os.path.join(self.image_dir, image_name)\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Encode question to indices\n",
        "        words = question.lower().split()\n",
        "        question_encoded = [self.vocab.get(word, self.vocab['<UNK>']) for word in words]\n",
        "\n",
        "        # Pad or truncate to max_seq_length\n",
        "        if len(question_encoded) < self.max_seq_length:\n",
        "            question_encoded += [self.vocab['<PAD>']] * (self.max_seq_length - len(question_encoded))\n",
        "        else:\n",
        "            question_encoded = question_encoded[:self.max_seq_length]\n",
        "\n",
        "        # Convert to tensors\n",
        "        question_tensor = torch.tensor(question_encoded, dtype=torch.long)\n",
        "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "        return image, question_tensor, label_tensor\n",
        "\n",
        "# Define image transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),           # Resize to 224x224 (ResNet input size)\n",
        "    transforms.ToTensor(),                   # Convert to tensor [0, 1]\n",
        "    transforms.Normalize(                    # Normalize with ImageNet stats\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = VQADataset(train, vocab, IMAGE_DIR, transform=transform)\n",
        "val_dataset = VQADataset(val, vocab, IMAGE_DIR, transform=transform)\n",
        "test_dataset = VQADataset(test, vocab, IMAGE_DIR, transform=transform)\n",
        "\n",
        "print(\"-\"*30)\n",
        "print(\"Datasets Created\")\n",
        "print(\"-\"*30)\n",
        "print(f\"Train: {len(train_dataset)} samples\")\n",
        "print(f\"Val:   {len(val_dataset)} samples\")\n",
        "print(f\"Test:  {len(test_dataset)} samples\")\n",
        "\n",
        "# Test loading one sample\n",
        "sample_image, sample_question, sample_label = train_dataset[0]\n",
        "print(f\"\\nSample data shapes:\")\n",
        "print(f\"  Image: {sample_image.shape}\")        # Should be [3, 224, 224]\n",
        "print(f\"  Question: {sample_question.shape}\")  # Should be [20]\n",
        "print(f\"  Label: {sample_label.item()}\")       # Should be 0 or 1\n",
        "print(\"-\"*30)"
      ],
      "metadata": {
        "id": "pYnkjfA6ikFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Dataloaders**"
      ],
      "metadata": {
        "id": "0-eiM7zllneT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size for training\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,        # Shuffle training data each epoch\n",
        "    num_workers=2,       # Parallel data loading\n",
        "    pin_memory=True      # Faster data transfer to GPU\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,       # Don't shuffle validation\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,       # Don't shuffle test\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(\"-\"*30)\n",
        "print(\"Dataloaders Created\")\n",
        "print(\"-\"*30)\n",
        "print(f\"Train batches: {len(train_loader)} (batch size: {BATCH_SIZE})\")\n",
        "print(f\"Val batches:   {len(val_loader)}\")\n",
        "print(f\"Test batches:  {len(test_loader)}\")\n",
        "\n",
        "# Test loading one batch\n",
        "sample_batch = next(iter(train_loader))\n",
        "batch_images, batch_questions, batch_labels = sample_batch\n",
        "\n",
        "print(f\"\\nSample batch shapes:\")\n",
        "print(f\"  Images: {batch_images.shape}\")        # [32, 3, 224, 224]\n",
        "print(f\"  Questions: {batch_questions.shape}\")  # [32, 20]\n",
        "print(f\"  Labels: {batch_labels.shape}\")        # [32]\n",
        "print(\"-\"*30)"
      ],
      "metadata": {
        "id": "5Ykh0IPykeNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**CNN Model Architecture (ResNet18 + BiLSTM)**"
      ],
      "metadata": {
        "id": "hGezVSwyntdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "\n",
        "class CNNBaselineVQA(nn.Module):\n",
        "    \"\"\"CNN Baseline: ResNet18 image encoder + BiLSTM question encoder\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim=256, lstm_hidden=512, img_feature_dim=512):\n",
        "        super(CNNBaselineVQA, self).__init__()\n",
        "\n",
        "        # Image encoder: Pre-trained ResNet18 (remove final FC layer)\n",
        "        resnet = models.resnet18(pretrained=True)\n",
        "        self.image_encoder = nn.Sequential(*list(resnet.children())[:-1])  # Output: 512-dim\n",
        "\n",
        "        # Question encoder: Embedding + BiLSTM\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(\n",
        "            embed_dim,\n",
        "            lstm_hidden,\n",
        "            batch_first=True,\n",
        "            bidirectional=True  # BiLSTM: 2 * lstm_hidden output\n",
        "        )\n",
        "\n",
        "        # Fusion and classification\n",
        "        fusion_dim = img_feature_dim + (lstm_hidden * 2)  # 512 + 1024 = 1536\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(fusion_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 2)  # Binary classification: yes/no\n",
        "        )\n",
        "\n",
        "    def forward(self, images, questions):\n",
        "        # Extract image features\n",
        "        img_features = self.image_encoder(images)  # [batch, 512, 1, 1]\n",
        "        img_features = img_features.view(img_features.size(0), -1)  # [batch, 512]\n",
        "\n",
        "        # Extract question features\n",
        "        embedded = self.embedding(questions)  # [batch, seq_len, embed_dim]\n",
        "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
        "        # Concatenate forward and backward hidden states\n",
        "        question_features = torch.cat([hidden[0], hidden[1]], dim=1)  # [batch, 1024]\n",
        "\n",
        "        # Fuse image and question features\n",
        "        combined = torch.cat([img_features, question_features], dim=1)  # [batch, 1536]\n",
        "\n",
        "        # Classify\n",
        "        output = self.classifier(combined)  # [batch, 2]\n",
        "        return output\n",
        "\n",
        "# Create model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = CNNBaselineVQA(vocab_size=len(vocab)).to(device)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"-\"*30)\n",
        "print(\"Modal Created\")\n",
        "print(\"-\"*30)\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"Device: {device}\")\n",
        "print(\"-\"*30)\n",
        "\n",
        "# Test forward pass\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_images = batch_images.to(device)\n",
        "    test_questions = batch_questions.to(device)\n",
        "    test_output = model(test_images, test_questions)\n",
        "    print(f\"\\nTest forward pass:\")\n",
        "    print(f\"  Input images: {test_images.shape}\")\n",
        "    print(f\"  Input questions: {test_questions.shape}\")\n",
        "    print(f\"  Output logits: {test_output.shape}\")  # [32, 2]\n",
        "print(\"-\"*30)"
      ],
      "metadata": {
        "id": "gtXIz1oLmdmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading the Best Model**"
      ],
      "metadata": {
        "id": "8fjOEF5iT5oD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load(f'{SAVE_DIR}/cnn_best_model.pth')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "print(f\"Loaded trained model: {checkpoint['val_acc']:.2f}% val accuracy\")"
      ],
      "metadata": {
        "id": "ySzSBfeUT-Ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Training & Evaulation functions**"
      ],
      "metadata": {
        "id": "Chcin3-Hoyi2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, questions, labels in tqdm(dataloader, desc=\"Training\"):\n",
        "        images = images.to(device)\n",
        "        questions = questions.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images, questions)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Statistics\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader)\n",
        "    epoch_acc = 100 * correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    \"\"\"Evaluate on validation/test set\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, questions, labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            images = images.to(device)\n",
        "            questions = questions.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images, questions)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader)\n",
        "    epoch_acc = 100 * correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "print(\"-\"*30)\n",
        "print(\"TRAINING FUNCTIONS DEFINED\")\n",
        "print(\"-\"*30)"
      ],
      "metadata": {
        "id": "figMvHEinqvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train CNN Baseline Model**"
      ],
      "metadata": {
        "id": "wz5T8JvhpObU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training configuration\n",
        "NUM_EPOCHS = 5\n",
        "LEARNING_RATE = 0.0001\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Track training history\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'train_acc': [],\n",
        "    'val_loss': [],\n",
        "    'val_acc': []\n",
        "}\n",
        "\n",
        "# Track best model\n",
        "best_val_acc = 0.0\n",
        "best_epoch = 0\n",
        "\n",
        "print(\"-\"*30)\n",
        "print(\"Training CNN Baseline Model\")\n",
        "print(\"-\"*30)\n",
        "print(f\"Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"Optimizer: Adam\")\n",
        "print(\"-\"*30)\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    print(f\"\\nEpoch {epoch}/{NUM_EPOCHS}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # Train\n",
        "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "\n",
        "    # Validate\n",
        "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
        "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_epoch = epoch\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_acc': val_acc,\n",
        "        }, f'{SAVE_DIR}/cnn_best_model.pth')\n",
        "        print(f\"*** Best model saved! (Val Acc: {val_acc:.2f}%) ***\")\n",
        "\n",
        "print(\"-\"*30)\n",
        "print(\"TRAINING COMPLETE\")\n",
        "print(\"-\"*30)\n",
        "print(f\"Best validation accuracy: {best_val_acc:.2f}% (Epoch {best_epoch})\")\n",
        "print(\"-\"*30)"
      ],
      "metadata": {
        "id": "l3Yi8Xrso4Lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaulate on TEST set**"
      ],
      "metadata": {
        "id": "qpiY2fAes6UQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Load best model\n",
        "checkpoint = torch.load(f'{SAVE_DIR}/cnn_best_model.pth')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "print(f\"Loaded best model from epoch {checkpoint['epoch']} (Val Acc: {checkpoint['val_acc']:.2f}%)\")\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"-\"*30)\n",
        "print(\"Test Set Evaluation\")\n",
        "print(\"-\"*30)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
        "print(\"-\"*30)\n",
        "\n",
        "# Get predictions for confusion matrix\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, questions, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        questions = questions.to(device)\n",
        "\n",
        "        outputs = model(images, questions)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "print(\"\\nConfusion Matrix\")\n",
        "print(\"-\"*30)\n",
        "print(\"              Predicted\")\n",
        "print(\"              No    Yes\")\n",
        "print(f\"Actual No     {cm[0][0]:<5} {cm[0][1]:<5}\")\n",
        "print(f\"Actual Yes    {cm[1][0]:<5} {cm[1][1]:<5}\")\n",
        "print(\"-\"*30)\n",
        "\n",
        "# Classification report\n",
        "report = classification_report(all_labels, all_preds, target_names=['No', 'Yes'], output_dict=True)\n",
        "print(\"\\nCalssification Report\")\n",
        "print(\"-\"*30)\n",
        "print(f\"Class 'No':  Precision={report['No']['precision']:.2f}, \"\n",
        "      f\"Recall={report['No']['recall']:.2f}, F1={report['No']['f1-score']:.2f}\")\n",
        "print(f\"Class 'Yes': Precision={report['Yes']['precision']:.2f}, \"\n",
        "      f\"Recall={report['Yes']['recall']:.2f}, F1={report['Yes']['f1-score']:.2f}\")\n",
        "print(f\"\\nAccuracy: {report['accuracy']:.2f}\")\n",
        "print(f\"Support No: {int(report['No']['support'])}, Support Yes: {int(report['Yes']['support'])}\")\n",
        "print(\"-\"*30)"
      ],
      "metadata": {
        "id": "LI1KMYfNpNW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Saving CNN Baseline Results**"
      ],
      "metadata": {
        "id": "6dmRy_hlul8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save complete results\n",
        "CNN_RESULTS = {\n",
        "    'model_name': 'CNN Baseline (ResNet18 + BiLSTM)',\n",
        "    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "\n",
        "    'architecture': {\n",
        "        'image_encoder': 'ResNet18 (pre-trained on ImageNet)',\n",
        "        'question_encoder': 'Embedding + BiLSTM',\n",
        "        'vocab_size': len(vocab),\n",
        "        'embedding_dim': 256,\n",
        "        'lstm_hidden': 512,\n",
        "        'total_params': total_params,\n",
        "        'trainable_params': trainable_params\n",
        "    },\n",
        "\n",
        "    'training': {\n",
        "        'epochs': NUM_EPOCHS,\n",
        "        'batch_size': BATCH_SIZE,\n",
        "        'learning_rate': LEARNING_RATE,\n",
        "        'optimizer': 'Adam',\n",
        "        'best_epoch': best_epoch,\n",
        "        'best_val_acc': round(best_val_acc, 2)\n",
        "    },\n",
        "\n",
        "    'dataset': {\n",
        "        'train_samples': len(train),\n",
        "        'val_samples': len(val),\n",
        "        'test_samples': len(test)\n",
        "    },\n",
        "\n",
        "    'test_results': {\n",
        "        'accuracy': round(test_acc, 2),\n",
        "        'loss': round(test_loss, 4),\n",
        "        'confusion_matrix': {\n",
        "            'true_no_pred_no': int(cm[0][0]),\n",
        "            'true_no_pred_yes': int(cm[0][1]),\n",
        "            'true_yes_pred_no': int(cm[1][0]),\n",
        "            'true_yes_pred_yes': int(cm[1][1])\n",
        "        },\n",
        "        'classification_report': {\n",
        "            'no_precision': round(report['No']['precision'], 2),\n",
        "            'no_recall': round(report['No']['recall'], 2),\n",
        "            'no_f1': round(report['No']['f1-score'], 2),\n",
        "            'yes_precision': round(report['Yes']['precision'], 2),\n",
        "            'yes_recall': round(report['Yes']['recall'], 2),\n",
        "            'yes_f1': round(report['Yes']['f1-score'], 2),\n",
        "            'support_no': int(report['No']['support']),\n",
        "            'support_yes': int(report['Yes']['support'])\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save results JSON\n",
        "with open(f'{SAVE_DIR}/CNN_BASELINE_RESULTS.json', 'w') as f:\n",
        "    json.dump(CNN_RESULTS, f, indent=2)\n",
        "\n",
        "# Save vocabulary\n",
        "with open(f'{SAVE_DIR}/vocabulary.pkl', 'wb') as f:\n",
        "    pickle.dump(vocab, f)\n",
        "\n",
        "# Save predictions\n",
        "with open(f'{SAVE_DIR}/cnn_test_predictions.pkl', 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'predictions': all_preds,\n",
        "        'ground_truth': all_labels\n",
        "    }, f)\n",
        "\n",
        "print(\"-\"*30)\n",
        "print(\"CNN Baseline Results Saved\")\n",
        "print(\"-\"*30)\n",
        "print(f\"Location: {SAVE_DIR}\")\n",
        "print(\"Files saved:\")\n",
        "print(\"  - CNN_BASELINE_RESULTS.json\")\n",
        "print(\"  - cnn_best_model.pth\")\n",
        "print(\"  - vocabulary.pkl\")\n",
        "print(\"  - cnn_test_predictions.pkl\")\n",
        "print(\"-\"*30)\n",
        "print(\"\\nCNN Baseline Locked:\")\n",
        "print(f\"  Test Accuracy: {test_acc:.2f}%\")\n",
        "print(f\"  Validation Accuracy: {best_val_acc:.2f}%\")\n",
        "print(f\"  Test Samples: {len(test)}\")\n",
        "print(\"-\"*30)"
      ],
      "metadata": {
        "id": "0oHgf5H9uiuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install BLIP-2 Dependencies**\n",
        "\n"
      ],
      "metadata": {
        "id": "QDKX5iwKTP0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.36.2 --break-system-packages --quiet\n",
        "!pip install accelerate --break-system-packages --quiet\n",
        "!pip install bitsandbytes --break-system-packages --quiet\n",
        "\n",
        "print(\"-\"*30)\n",
        "print(\"BLIP-2 Dependencies Installed\")\n",
        "print(\"-\"*30)"
      ],
      "metadata": {
        "id": "gDC013A4Tguu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading BLIP-2 Model**"
      ],
      "metadata": {
        "id": "P-s3a14NWp5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
        "from PIL import Image\n",
        "\n",
        "print(\"-\"*30)\n",
        "print(\"Loading BLIP-2 Model\")\n",
        "print(\"-\"*30)\n",
        "print(\"Model: Salesforce/blip2-opt-2.7b\")\n",
        "print(\"-\"*30)\n",
        "\n",
        "# Load processor (handles image + text preprocessing)\n",
        "processor = Blip2Processor.from_pretrained(\n",
        "    \"Salesforce/blip2-opt-2.7b\",\n",
        "    use_fast=False  # Avoid tokenizer serialization bug\n",
        ")\n",
        "\n",
        "# Load model\n",
        "blip2_model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip2-opt-2.7b\",\n",
        "    torch_dtype=torch.float16,  # FP16 for memory efficiency\n",
        "    device_map=\"auto\"            # Auto device placement\n",
        ")\n",
        "\n",
        "print(\"-\"*30)\n",
        "print(\"BLIP-2 Model Loaded\")\n",
        "print(\"-\"*30)\n",
        "print(f\"Model: BLIP-2 OPT-2.7B\")\n",
        "print(f\"Parameters: 2.7B (frozen)\")\n",
        "print(f\"Device: {blip2_model.device}\")\n",
        "print(\"-\"*30)"
      ],
      "metadata": {
        "id": "YD0VU869U_d0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BLIP-2 VQA Prediction Function**"
      ],
      "metadata": {
        "id": "2keFrZk6YGLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def blip2_vqa_predict(image, question, model, processor):\n",
        "    \"\"\"\n",
        "    Zero-shot VQA using BLIP-2\n",
        "\n",
        "    Args:\n",
        "        image: PIL Image\n",
        "        question: str\n",
        "        model: BLIP-2 model\n",
        "        processor: BLIP-2 processor\n",
        "\n",
        "    Returns:\n",
        "        str: Predicted answer\n",
        "    \"\"\"\n",
        "    # Format prompt for VQA\n",
        "    prompt = f\"Question: {question} Answer:\"\n",
        "\n",
        "    # Preprocess inputs\n",
        "    inputs = processor(\n",
        "        images=image,\n",
        "        text=prompt,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device, torch.float16)\n",
        "\n",
        "    # Generate answer\n",
        "    generated_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=10,   # Short answers for yes/no\n",
        "        num_beams=1,         # Greedy decoding\n",
        "        do_sample=False      # Deterministic\n",
        "    )\n",
        "\n",
        "    # Decode answer\n",
        "    generated_text = processor.batch_decode(\n",
        "        generated_ids,\n",
        "        skip_special_tokens=True\n",
        "    )[0].strip()\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "print(\"-\"*30)\n",
        "print(\"BLIP-2 VQA Function Defined\")\n",
        "print(\"-\"*30)\n",
        "\n",
        "# Test on one sample\n",
        "sample_row = test.iloc[0]\n",
        "sample_image_path = os.path.join(IMAGE_DIR, sample_row['image_name'])\n",
        "sample_image = Image.open(sample_image_path).convert('RGB')\n",
        "sample_question = sample_row['question']\n",
        "sample_answer = sample_row['answer']\n",
        "\n",
        "test_prediction = blip2_vqa_predict(sample_image, sample_question, blip2_model, processor)\n",
        "\n",
        "print(f\"\\nTest prediction:\")\n",
        "print(f\"  Question: {sample_question}\")\n",
        "print(f\"  Ground Truth: {sample_answer}\")\n",
        "print(f\"  BLIP-2 Prediction: {test_prediction}\")\n",
        "print(\"-\"*30)"
      ],
      "metadata": {
        "id": "Ny0YctlWWs33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate BLIP-2 Zero-Shot on Test Set**"
      ],
      "metadata": {
        "id": "YmN1YleXYqvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"-\"*30)\n",
        "print(\"Evaluating BLIP-2 ZERO-SHOT on Test Set\")\n",
        "print(\"-\"*30)\n",
        "print(f\"Test samples: {len(test)}\")\n",
        "print(\"-\"*30)\n",
        "\n",
        "blip2_predictions = []\n",
        "blip2_ground_truth = []\n",
        "blip2_raw_outputs = []\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "for idx, row in tqdm(test.iterrows(), total=len(test), desc=\"Evaluating BLIP-2\"):\n",
        "    # Load image\n",
        "    image_path = os.path.join(IMAGE_DIR, row['image_name'])\n",
        "    try:\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "    except:\n",
        "        print(f\"Warning: Could not load {row['image_name']}\")\n",
        "        continue\n",
        "\n",
        "    # Get question and ground truth\n",
        "    question = row['question']\n",
        "    ground_truth = row['answer'].lower().strip()\n",
        "\n",
        "    # Get BLIP-2 prediction\n",
        "    prediction = blip2_vqa_predict(image, question, blip2_model, processor)\n",
        "    prediction_lower = prediction.lower().strip()\n",
        "\n",
        "    # Store results\n",
        "    blip2_predictions.append(prediction_lower)\n",
        "    blip2_ground_truth.append(ground_truth)\n",
        "    blip2_raw_outputs.append(prediction)\n",
        "\n",
        "    # Check if correct (flexible matching for yes/no)\n",
        "    is_correct = False\n",
        "    if ground_truth == 'yes':\n",
        "        is_correct = 'yes' in prediction_lower\n",
        "    elif ground_truth == 'no':\n",
        "        is_correct = 'no' in prediction_lower and 'yes' not in prediction_lower\n",
        "\n",
        "    if is_correct:\n",
        "        correct += 1\n",
        "\n",
        "    total += 1\n",
        "\n",
        "# Calculate accuracy\n",
        "blip2_accuracy = (correct / total) * 100\n",
        "\n",
        "print(\"-\"*30)\n",
        "print(\"BLIP-2 ZERO-SHOT Evaluation Complete\")\n",
        "print(\"-\"*30)\n",
        "print(f\"Total samples: {total}\")\n",
        "print(f\"Correct predictions: {correct}\")\n",
        "print(f\"Accuracy: {blip2_accuracy:.2f}%\")\n",
        "print(\"-\"*30)"
      ],
      "metadata": {
        "id": "TtwDQQv3YZQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Caclculate BLIP-2 Detailed Metrics**"
      ],
      "metadata": {
        "id": "dDtIdX1waW9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "print(\"-\"*30)\n",
        "print(\"Calculating BLIP-2 Metrics\")\n",
        "print(\"-\"*30)\n",
        "\n",
        "# Convert to binary labels (0=no, 1=yes)\n",
        "blip2_pred_binary = []\n",
        "blip2_true_binary = []\n",
        "\n",
        "for pred, true in zip(blip2_predictions, blip2_ground_truth):\n",
        "    # Ground truth\n",
        "    true_label = 1 if true == 'yes' else 0\n",
        "    blip2_true_binary.append(true_label)\n",
        "\n",
        "    # Prediction (flexible matching)\n",
        "    if 'yes' in pred and 'no' not in pred:\n",
        "        pred_label = 1\n",
        "    elif 'no' in pred:\n",
        "        pred_label = 0\n",
        "    else:\n",
        "        pred_label = 0  # Default to 'no' if unclear\n",
        "\n",
        "    blip2_pred_binary.append(pred_label)\n",
        "\n",
        "# Confusion matrix\n",
        "cm_blip2 = confusion_matrix(blip2_true_binary, blip2_pred_binary)\n",
        "\n",
        "print(\"\\nConfusion Matrix - BLIP-2 ZERO-SHOT\")\n",
        "print(\"-\"*30)\n",
        "print(\"              Predicted\")\n",
        "print(\"              No    Yes\")\n",
        "print(f\"Actual No     {cm_blip2[0][0]:<5} {cm_blip2[0][1]:<5}\")\n",
        "print(f\"Actual Yes    {cm_blip2[1][0]:<5} {cm_blip2[1][1]:<5}\")\n",
        "print(\"-\"*30)\n",
        "\n",
        "# Classification report\n",
        "report_blip2 = classification_report(\n",
        "    blip2_true_binary,\n",
        "    blip2_pred_binary,\n",
        "    target_names=['No', 'Yes'],\n",
        "    output_dict=True\n",
        ")\n",
        "\n",
        "print(\"\\nClassification Report\")\n",
        "print(\"-\"*30)\n",
        "print(f\"Class 'No':  Precision={report_blip2['No']['precision']:.2f}, \"\n",
        "      f\"Recall={report_blip2['No']['recall']:.2f}, F1={report_blip2['No']['f1-score']:.2f}\")\n",
        "print(f\"Class 'Yes': Precision={report_blip2['Yes']['precision']:.2f}, \"\n",
        "      f\"Recall={report_blip2['Yes']['recall']:.2f}, F1={report_blip2['Yes']['f1-score']:.2f}\")\n",
        "print(f\"\\nAccuracy: {report_blip2['accuracy']:.2f}\")\n",
        "print(f\"Support No: {int(report_blip2['No']['support'])}, Support Yes: {int(report_blip2['Yes']['support'])}\")\n",
        "print(\"-\"*30)"
      ],
      "metadata": {
        "id": "xOGCFZe-ZDiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save BLIP-2 Results & Comparison**"
      ],
      "metadata": {
        "id": "3G829as1bE8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "blip2_accuracy_corrected = report_blip2['accuracy'] * 100\n",
        "\n",
        "# Save BLIP-2 results\n",
        "BLIP2_RESULTS = {\n",
        "    'model_name': 'BLIP-2 Zero-Shot (OPT-2.7B)',\n",
        "    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "\n",
        "    'architecture': {\n",
        "        'vision_encoder': 'EVA-CLIP (ViT-g/14)',\n",
        "        'language_model': 'OPT-2.7B',\n",
        "        'total_params': '2.7B',\n",
        "        'approach': 'zero-shot (no training)'\n",
        "    },\n",
        "\n",
        "    'dataset': {\n",
        "        'test_samples': total\n",
        "    },\n",
        "\n",
        "    'test_results': {\n",
        "        'accuracy': round(blip2_accuracy_corrected, 2),\n",
        "        'confusion_matrix': {\n",
        "            'true_no_pred_no': int(cm_blip2[0][0]),\n",
        "            'true_no_pred_yes': int(cm_blip2[0][1]),\n",
        "            'true_yes_pred_no': int(cm_blip2[1][0]),\n",
        "            'true_yes_pred_yes': int(cm_blip2[1][1])\n",
        "        },\n",
        "        'classification_report': {\n",
        "            'no_precision': round(report_blip2['No']['precision'], 2),\n",
        "            'no_recall': round(report_blip2['No']['recall'], 2),\n",
        "            'no_f1': round(report_blip2['No']['f1-score'], 2),\n",
        "            'yes_precision': round(report_blip2['Yes']['precision'], 2),\n",
        "            'yes_recall': round(report_blip2['Yes']['recall'], 2),\n",
        "            'yes_f1': round(report_blip2['Yes']['f1-score'], 2),\n",
        "            'support_no': int(report_blip2['No']['support']),\n",
        "            'support_yes': int(report_blip2['Yes']['support'])\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save BLIP-2 results\n",
        "with open(f'{SAVE_DIR}/BLIP2_BASELINE_RESULTS.json', 'w') as f:\n",
        "    json.dump(BLIP2_RESULTS, f, indent=2)\n",
        "\n",
        "with open(f'{SAVE_DIR}/blip2_test_predictions.pkl', 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'predictions': blip2_predictions,\n",
        "        'predictions_binary': blip2_pred_binary,\n",
        "        'ground_truth': blip2_ground_truth,\n",
        "        'ground_truth_binary': blip2_true_binary,\n",
        "        'raw_outputs': blip2_raw_outputs\n",
        "    }, f)\n",
        "\n",
        "# Load CNN results for comparison\n",
        "with open(f'{SAVE_DIR}/CNN_BASELINE_RESULTS.json', 'r') as f:\n",
        "    cnn_results = json.load(f)\n",
        "\n",
        "print(\"-\"*70)\n",
        "print(\"FINAL COMPARISON: CNN vs BLIP-2\")\n",
        "print(\"-\"*70)\n",
        "print(f\"{'Metric':<30} {'CNN Baseline':<20} {'BLIP-2 Zero-Shot':<20}\")\n",
        "print(\"-\"*70)\n",
        "print(f\"{'Approach':<30} {'Trained (908 samples)':<20} {'Zero-shot':<20}\")\n",
        "print(f\"{'Parameters':<30} {'15.2M total':<20} {'2.7B frozen':<20}\")\n",
        "print(\"-\"*70)\n",
        "print(f\"{'Test Accuracy':<30} {cnn_results['test_results']['accuracy']:<20.2f} {blip2_accuracy_corrected:<20.2f}\")\n",
        "print(f\"{'Difference':<30} {'-':<20} {blip2_accuracy_corrected - cnn_results['test_results']['accuracy']:+.2f}\")\n",
        "print(\"-\"*70)\n",
        "print(f\"{'Precision (No)':<30} {cnn_results['test_results']['classification_report']['no_precision']:<20.2f} {report_blip2['No']['precision']:<20.2f}\")\n",
        "print(f\"{'Precision (Yes)':<30} {cnn_results['test_results']['classification_report']['yes_precision']:<20.2f} {report_blip2['Yes']['precision']:<20.2f}\")\n",
        "print(\"-\"*70)\n",
        "print(f\"{'Recall (No)':<30} {cnn_results['test_results']['classification_report']['no_recall']:<20.2f} {report_blip2['No']['recall']:<20.2f}\")\n",
        "print(f\"{'Recall (Yes)':<30} {cnn_results['test_results']['classification_report']['yes_recall']:<20.2f} {report_blip2['Yes']['recall']:<20.2f}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "# Save comparison\n",
        "comparison = {\n",
        "    'cnn_baseline': cnn_results['test_results'],\n",
        "    'blip2_zero_shot': BLIP2_RESULTS['test_results'],\n",
        "    'key_findings': {\n",
        "        'accuracy_difference': round(blip2_accuracy_corrected - cnn_results['test_results']['accuracy'], 2),\n",
        "        'winner': 'CNN' if cnn_results['test_results']['accuracy'] > blip2_accuracy_corrected else 'BLIP-2',\n",
        "        'cnn_bias': 'Balanced predictions',\n",
        "        'blip2_bias': 'Biased toward Yes (81% recall)'\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(f'{SAVE_DIR}/BASELINE_COMPARISON.json', 'w') as f:\n",
        "    json.dump(comparison, f, indent=2)\n",
        "\n",
        "print(\"\\nALL RESULTS SAVED:\")\n",
        "print(f\"  {SAVE_DIR}BLIP2_BASELINE_RESULTS.json\")\n",
        "print(f\"  {SAVE_DIR}BASELINE_COMPARISON.json\")\n",
        "print(\"-\"*30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75xXybosauuD",
        "outputId": "297571d6-37b7-4731-b328-c4c30f1a7e99"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------\n",
            "FINAL COMPARISON: CNN vs BLIP-2\n",
            "----------------------------------------------------------------------\n",
            "Metric                         CNN Baseline         BLIP-2 Zero-Shot    \n",
            "----------------------------------------------------------------------\n",
            "Approach                       Trained (908 samples) Zero-shot           \n",
            "Parameters                     15.2M total          2.7B frozen         \n",
            "----------------------------------------------------------------------\n",
            "Test Accuracy                  72.82                59.49               \n",
            "Difference                     -                    -13.33\n",
            "----------------------------------------------------------------------\n",
            "Precision (No)                 0.75                 0.73                \n",
            "Precision (Yes)                0.70                 0.53                \n",
            "----------------------------------------------------------------------\n",
            "Recall (No)                    0.75                 0.42                \n",
            "Recall (Yes)                   0.70                 0.81                \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "ALL RESULTS SAVED:\n",
            "  /content/drive/MyDrive/alternative-assessment-materials/VQA_RAD_LOCKED_RESULTS/BLIP2_BASELINE_RESULTS.json\n",
            "  /content/drive/MyDrive/alternative-assessment-materials/VQA_RAD_LOCKED_RESULTS/BASELINE_COMPARISON.json\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z15-f-8ceVyz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}