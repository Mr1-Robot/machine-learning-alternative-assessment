{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNLcLlc71dp1zZiosVy15bY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mr1-Robot/machine-learning-alternative-assessment/blob/main/alternative_assessment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Alternative Assessment - WOA7015\n",
        "#####MUAAMAR MOHAMMED ABDULLAH AL-GHRAIRI - 24084470"
      ],
      "metadata": {
        "id": "d3YMC15UZMvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    print(f\"Random Seed set to {seed}. Results are now reproducible.\")\n",
        "\n",
        "seed_everything(42)"
      ],
      "metadata": {
        "id": "AFLIcr3WS7b4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8KlzI69XiO5"
      },
      "outputs": [],
      "source": [
        "# Mount google drive to access the dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DATA_ROOT = \"/content/drive/MyDrive/alternative-assessment-materials/\"\n",
        "IMAGE_DIR = f\"{DATA_ROOT}VQA_RAD_Images\"\n",
        "DATASET = f\"{DATA_ROOT}VQA_RAD_Dataset.json\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Core libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Data processing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Utilities\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"-\"*30)\n",
        "print(\"SETUP\")\n",
        "print(\"-\"*30)\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "print(\"-\"*30)"
      ],
      "metadata": {
        "id": "TF88Uj5tYtHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load & Explore **Dataset**"
      ],
      "metadata": {
        "id": "lDH11gZfdFB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "vqa_data = pd.read_json(DATASET)\n",
        "# Normalize the 'answer_type', since it has 'CLOSED' and 'CLOSED ' <- extra space at the end\n",
        "vqa_data['answer_type'] = vqa_data['answer_type'].str.strip().str.upper()\n",
        "\n",
        "print(f\"Total Q&A pairs: {len(vqa_data)}\")\n",
        "print(f\"Columns: {vqa_data.columns.tolist()}\")\n",
        "\n",
        "# # Answer type distribution\n",
        "answer_types = vqa_data['answer_type'].value_counts()\n",
        "print(\"\\nAnswer Type Distribution:\")\n",
        "for key, value in answer_types.items():\n",
        "  print(f\"{key}: {value} ({value/len(vqa_data)*100:.1f}%)\")\n",
        "\n",
        "# Closed-ended questions\n",
        "closed_ended_data = vqa_data[vqa_data['answer_type'] == 'CLOSED'].copy()\n",
        "print(f\"\\nClosed-Ended Questions: {len(closed_ended_data)}\\n\")\n",
        "\n",
        "# Normalize answers, adding extra key 'answer_norm'\n",
        "closed_ended_data['answer_norm'] = closed_ended_data['answer'].str.lower().str.strip()\n",
        "print(f\"Normalized answers: {closed_ended_data['answer_norm']}\")\n",
        "\n",
        "# Closed answers distribution\n",
        "closed_answers_dist = closed_ended_data['answer_norm'].value_counts()\n",
        "print(f\"\\nClosed Answers Distribution: {closed_answers_dist}\")\n",
        "for answer, count in closed_answers_dist.items():\n",
        "    print(f\"{answer}: {count} ({count/len(closed_ended_data)*100:.1f}%)\")\n",
        "\n",
        "# Organ distribution\n",
        "organ_dist = closed_ended_data['image_organ'].value_counts()\n",
        "print(\"\\nImage Organ Distribution:\")\n",
        "for organ, count in organ_dist.items():\n",
        "    print(f\"{organ}: {count} ({count/len(vqa_data)*100:.1f}%)\")"
      ],
      "metadata": {
        "id": "g8DtBq2-bgik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Constructing Binary (Yes/No) Dataset**"
      ],
      "metadata": {
        "id": "gTdbqQRwCIls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"-\"*30)\n",
        "print(\"Constructing Binary (Yes/No) Dataset\")\n",
        "print(\"-\"*30)\n",
        "\n",
        "# Filter for yes/no answers only\n",
        "binary_data = closed_ended_data[\n",
        "    closed_ended_data['answer_norm'].isin(['yes', 'no'])\n",
        "].copy()\n",
        "\n",
        "\n",
        "# Create binary labels for yes=1, no=0\n",
        "binary_data['label'] = (binary_data['answer_norm'] == 'yes').astype(float)\n",
        "\n",
        "print(f\"Binary Dataset Size: {len(binary_data)}\")\n",
        "\n",
        "print(\"\\nLabel Distrubution:\")\n",
        "label_dist = binary_data['label'].value_counts().sort_index()\n",
        "for label, count in label_dist.items():\n",
        "  answer = 'yes' if label == 1 else 'no'\n",
        "  print(f\"  {label} ({answer}): {count} ({count/len(binary_data)*100:.1f}%)\")\n",
        "\n",
        "# Check organ distribution in binary subset\n",
        "print(f\"\\nOrgan Distribution in Binary Dataset:\")\n",
        "organ_dist = binary_data['image_organ'].value_counts()\n",
        "for organ, count in organ_dist.items():\n",
        "    print(f\"  {organ}: {count} ({count/len(binary_data)*100:.1f}%)\")\n",
        "\n",
        "# Check question type distribution\n",
        "print(f\"\\nQuestion Type Distribution:\")\n",
        "question_types = binary_data['question_type'].value_counts()\n",
        "for question_type, count in question_types.items():\n",
        "    print(f\"  {question_type}: {count} ({count/len(binary_data)*100:.1f}%)\")\n",
        "\n",
        "# Verify no missing images\n",
        "binary_data['image_path'] = binary_data['image_name'].apply(\n",
        "    lambda x: os.path.join(IMAGE_DIR, x)\n",
        ")\n",
        "missing_images = binary_data[~binary_data['image_path'].apply(os.path.exists)]\n",
        "print(f\"\\nMissing images: {len(missing_images)}\")\n",
        "\n",
        "print(f\"\\nFinal Binary Dataset: {len(binary_data)} examples\")\n"
      ],
      "metadata": {
        "id": "yyt08zipd5Ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Create PyTorch Dataset**"
      ],
      "metadata": {
        "id": "MSyi-Qehitho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import re\n",
        "\n",
        "# Dataset Class\n",
        "class VQADataset(Dataset):\n",
        "    \"\"\"\n",
        "    VQA Dataset for binary yes/no classification.\n",
        "    Returns: (image, question_indices, question_len, label)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataframe, image_dir, vocab, transform=None, max_seq_len=20):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dataframe: DataFrame with 'image_name', 'question', 'label'\n",
        "            image_dir: Path to images\n",
        "            vocab: Dictionary mapping words to integers {'word': idx}\n",
        "            transform: Image transforms\n",
        "            max_seq_len: Fixed length for padding (default 20)\n",
        "        \"\"\"\n",
        "        self.data = dataframe.reset_index(drop=True)\n",
        "        self.image_dir = image_dir\n",
        "        self.vocab = vocab\n",
        "        self.transform = transform\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        # Convert to lower case and remove punctuation\n",
        "        text = str(text).lower()\n",
        "        tokens = re.findall(r'\\b\\w+\\b', text)\n",
        "        return tokens\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "\n",
        "        # LOAD IMAGE\n",
        "        image_path = os.path.join(self.image_dir, row['image_name'])\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # PROCESS QUESTION\n",
        "        tokens = self.tokenize(row['question'])\n",
        "\n",
        "        # Handle empty questions\n",
        "        # If tokenizer returns empty list (e.g., question was just \"?\"), use <UNK>\n",
        "        if len(tokens) == 0:\n",
        "            tokens = ['<UNK>']\n",
        "\n",
        "        # Convert tokens to indices\n",
        "        indices = [self.vocab.get(token, self.vocab.get('<UNK>', 1)) for token in tokens]\n",
        "\n",
        "        # Truncate if too long\n",
        "        if len(indices) > self.max_seq_len:\n",
        "            indices = indices[:self.max_seq_len]\n",
        "\n",
        "        actual_length = len(indices)\n",
        "\n",
        "        # Pad with zeros (or <PAD> token index)\n",
        "        pad_len = self.max_seq_len - len(indices)\n",
        "        indices += [self.vocab.get('<PAD>', 0)] * pad_len\n",
        "\n",
        "        # CONVERT TO TENSORS\n",
        "        q_tensor = torch.tensor(indices, dtype=torch.long)\n",
        "        len_tensor = torch.tensor(actual_length, dtype=torch.long)\n",
        "        label_tensor = torch.tensor(row['label'], dtype=torch.float32)\n",
        "\n",
        "        return image, q_tensor, len_tensor, label_tensor\n",
        "\n",
        "\n",
        "# Define transforms (Standard Medical/ImageNet preprocessing)\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "print(\"VQADataset defined (with Tokenization & Padding logic)\")\n",
        "print(\"Transforms defined\")"
      ],
      "metadata": {
        "id": "pYnkjfA6ikFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group questions by image to see the structure\n",
        "image_qa_mapping = binary_data.groupby('image_name').apply(\n",
        "    lambda group: [\n",
        "        {\n",
        "            'question': row['question'],\n",
        "            'answer': row['answer_norm'],\n",
        "            'label': row['label']\n",
        "        }\n",
        "        for _, row in group.iterrows()\n",
        "    ]\n",
        ").to_dict()\n",
        "\n",
        "print(f\"Total unique images: {len(image_qa_mapping)}\")\n",
        "print(f\"Total Q&A pairs: {len(binary_data)}\")\n",
        "print(f\"Avg questions per image: {len(binary_data)/len(image_qa_mapping):.2f}\")\n",
        "\n",
        "# Show distribution of questions per image\n",
        "qa_per_image = binary_data.groupby('image_name').size()\n",
        "print(f\"\\nQuestions per image distribution:\")\n",
        "print(f\"  Min: {qa_per_image.min()}\")\n",
        "print(f\"  Max: {qa_per_image.max()}\")\n",
        "print(f\"  Median: {qa_per_image.median():.0f}\")\n",
        "print(f\"  Mean: {qa_per_image.mean():.2f}\")\n",
        "\n",
        "# Sample a few images to inspect\n",
        "print(\"\\n\" + \"-\"*60)\n",
        "print(\"SAMPLE IMAGE-QUESTION MAPPINGS\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "sample_images = list(image_qa_mapping.keys())[:5]\n",
        "for img_name in sample_images:\n",
        "    qa_pairs = image_qa_mapping[img_name]\n",
        "    print(f\"\\n{img_name}\")\n",
        "    print(f\"   Organ: {binary_data[binary_data['image_name']==img_name]['image_organ'].iloc[0]}\")\n",
        "    print(f\"   Questions ({len(qa_pairs)}):\")\n",
        "    for i, qa in enumerate(qa_pairs, 1):\n",
        "        print(f\"   {i}. Q: {qa['question']}\")\n",
        "        print(f\"      A: {qa['answer']} (label={qa['label']})\")"
      ],
      "metadata": {
        "id": "bEN2UDkbAsbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train/Val/Test Split (Stratified)**"
      ],
      "metadata": {
        "id": "0-eiM7zllneT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Get unique images\n",
        "unique_images = binary_data['image_name'].unique()\n",
        "labels_per_image = binary_data.groupby('image_name')['label'].mean()\n",
        "\n",
        "# Split IMAGES (80/10/10)\n",
        "train_imgs, temp_imgs = train_test_split(unique_images, test_size=0.2, random_state=42)\n",
        "val_imgs, test_imgs = train_test_split(temp_imgs, test_size=0.5, random_state=42)\n",
        "\n",
        "# 3. Assign DataFrame rows based on the image split\n",
        "train_df = binary_data[binary_data['image_name'].isin(train_imgs)].copy()\n",
        "val_df = binary_data[binary_data['image_name'].isin(val_imgs)].copy()\n",
        "test_df = binary_data[binary_data['image_name'].isin(test_imgs)].copy()\n",
        "\n",
        "print(\"Group-Aware Split (No image leakage):\")\n",
        "print(f\"  Train: {len(train_df)} questions ({len(train_imgs)} images)\")\n",
        "print(f\"  Val:   {len(val_df)} questions ({len(val_imgs)} images)\")\n",
        "print(f\"  Test:  {len(test_df)} questions ({len(test_imgs)} images)\")\n",
        "\n",
        "# Verify no overlap\n",
        "assert set(train_df['image_name']) & set(test_df['image_name']) == set(), \"FATAL: Train/Test overlap detected!\""
      ],
      "metadata": {
        "id": "5Ykh0IPykeNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build Vocabulary**"
      ],
      "metadata": {
        "id": "CmgaZz8-QC0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "def tokenize(text):\n",
        "  \"\"\"\n",
        "  Simple tokenizer: lowercase and split on non-alphanumeric\n",
        "  \"\"\"\n",
        "  text = text.lower()\n",
        "  tokens = re.findall(r'\\b\\w+\\b', text)\n",
        "  return tokens\n",
        "\n",
        "\n",
        "# Build vocabulary from training questions only\n",
        "all_tokens = []\n",
        "for question in train_df['question']:\n",
        "  all_tokens.extend(tokenize(question))\n",
        "\n",
        "token_counts = Counter(all_tokens)\n",
        "print(f\"Total tokens in training: {len(all_tokens)}\")\n",
        "print(f\"Unique tokens: {len(token_counts)}\")\n",
        "\n",
        "# Create vocabulary with special tokens\n",
        "vocab = {\n",
        "    '<PAD>': 0,\n",
        "    '<UNK>': 1,\n",
        "}\n",
        "\n",
        "# Add tokens that appear at leat min_freq times\n",
        "min_freq = 2\n",
        "for token, count in token_counts.items():\n",
        "  if count >= min_freq:\n",
        "    vocab[token] = len(vocab)\n",
        "\n",
        "print(f\"Vocabulary size (min_freq={min_freq}): {len(vocab)}\")\n",
        "print(f\"\\nMost common tokens:\")\n",
        "for token, count in token_counts.most_common(20):\n",
        "    print(f\"  '{token}': {count}\")\n",
        "\n",
        "# Create reverse mapping\n",
        "idx_to_token = {idx: token for token, idx in vocab.items()}"
      ],
      "metadata": {
        "id": "Y_kU6QRxQBT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Class**"
      ],
      "metadata": {
        "id": "CBIyUAhhTBI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VQABinaryDataset(Dataset):\n",
        "    def __init__(self, dataframe, image_dir, vocab, transform=None, max_seq_len=30):\n",
        "        self.data = dataframe.reset_index(drop=True)\n",
        "        self.image_dir = image_dir\n",
        "        self.vocab = vocab\n",
        "        self.transform = transform\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        text = text.lower()\n",
        "        tokens = re.findall(r'\\b\\w+\\b', text)\n",
        "        return tokens\n",
        "\n",
        "    def encode_question(self, question):\n",
        "        tokens = self.tokenize(question)\n",
        "\n",
        "        # Handle empty questions (e.g. \"?\") by inserting UNK\n",
        "        if len(tokens) == 0:\n",
        "            tokens = ['<UNK>']\n",
        "\n",
        "        # Convert tokens to indices\n",
        "        indices = [self.vocab.get(token, self.vocab['<UNK>']) for token in tokens]\n",
        "\n",
        "        # Truncate\n",
        "        if len(indices) > self.max_seq_len:\n",
        "            indices = indices[:self.max_seq_len]\n",
        "\n",
        "        actual_length = len(indices)\n",
        "\n",
        "        # Pad\n",
        "        if len(indices) < self.max_seq_len:\n",
        "            indices += [self.vocab['<PAD>']] * (self.max_seq_len - len(indices))\n",
        "\n",
        "        return torch.tensor(indices, dtype=torch.long), actual_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        image_path = os.path.join(self.image_dir, row['image_name'])\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        question_encoded, question_length = self.encode_question(row['question'])\n",
        "        label = torch.tensor(row['label'], dtype=torch.float32)\n",
        "\n",
        "        return image, question_encoded, question_length, label\n",
        "\n",
        "# Re-create datasets with the fix\n",
        "train_dataset = VQABinaryDataset(train_df, IMAGE_DIR, vocab, transform=train_transform)\n",
        "val_dataset = VQABinaryDataset(val_df, IMAGE_DIR, vocab, transform=val_transform)\n",
        "test_dataset = VQABinaryDataset(test_df, IMAGE_DIR, vocab, transform=val_transform)\n",
        "\n",
        "# Re-create loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "print(\"Datasets patched for zero-length sequences.\")"
      ],
      "metadata": {
        "id": "p8C5dsF0TDZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataloaders**"
      ],
      "metadata": {
        "id": "hOJeECuRTg1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoader configuration\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "print(f\"DataLoaders created:\")\n",
        "print(f\"  Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  Train batches: {len(train_loader)}\")\n",
        "print(f\"  Val batches:   {len(val_loader)}\")\n",
        "print(f\"  Test batches:  {len(test_loader)}\")\n",
        "\n",
        "# Test batch loading\n",
        "sample_batch = next(iter(train_loader))\n",
        "images, questions, lengths, labels = sample_batch\n",
        "print(f\"\\nSample batch:\")\n",
        "print(f\"  Images shape: {images.shape}\")\n",
        "print(f\"  Questions shape: {questions.shape}\")\n",
        "print(f\"  Lengths shape: {lengths.shape}\")\n",
        "print(f\"  Labels shape: {labels.shape}\")"
      ],
      "metadata": {
        "id": "tit0ca8wTgfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**CNN Model Architecture (ResNet18 + LSTM)**"
      ],
      "metadata": {
        "id": "hGezVSwyntdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "from torchvision.models import ResNet18_Weights\n",
        "\n",
        "class MultimodalVQAModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=300, lstm_hidden_dim=64, dropout=0.5):\n",
        "        super(MultimodalVQAModel, self).__init__()\n",
        "\n",
        "        # Vision: ResNet-18 (Frozen)\n",
        "        resnet = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
        "        self.vision_encoder = nn.Sequential(*list(resnet.children())[:-1])\n",
        "        for param in self.vision_encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Text: Small LSTM\n",
        "        self.question_embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.question_lstm = nn.LSTM(embedding_dim, lstm_hidden_dim, batch_first=True, bidirectional=True)\n",
        "\n",
        "        # Fusion\n",
        "        # Vision(512) + LSTM(64*2 = 128) = 640\n",
        "        self.fusion_dim = 512 + (lstm_hidden_dim * 2)\n",
        "\n",
        "        # 4. Classifier\n",
        "        # REMOVED Sigmoid from the end!\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.fusion_dim, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(32, 1) # Output raw logits\n",
        "        )\n",
        "\n",
        "    def forward(self, images, questions, question_lengths):\n",
        "        batch_size = images.size(0)\n",
        "\n",
        "        # Vision\n",
        "        vision_features = self.vision_encoder(images).view(batch_size, -1)\n",
        "\n",
        "        # Language\n",
        "        question_embedded = self.question_embedding(questions)\n",
        "        packed_questions = nn.utils.rnn.pack_padded_sequence(\n",
        "            question_embedded,\n",
        "            question_lengths.cpu(),\n",
        "            batch_first=True,\n",
        "            enforce_sorted=False\n",
        "        )\n",
        "        packed_output, (hidden, cell) = self.question_lstm(packed_questions)\n",
        "        question_features = torch.cat([hidden[0], hidden[1]], dim=1)\n",
        "\n",
        "        # Fusion\n",
        "        fused_features = torch.cat([vision_features, question_features], dim=1)\n",
        "\n",
        "        # Prediction (Logits)\n",
        "        predictions = self.classifier(fused_features)\n",
        "        return predictions.squeeze(1)\n",
        "\n",
        "# Re-initialize with SMALLER dimensions\n",
        "model = MultimodalVQAModel(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=300,\n",
        "    lstm_hidden_dim=64,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "print(\"Model re-initialized.\")\n",
        "\n",
        "# Test Forward Pass again\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_images, test_questions, test_lengths, test_labels = next(iter(train_loader))\n",
        "    test_images = test_images.to(device)\n",
        "    test_questions = test_questions.to(device)\n",
        "\n",
        "    # This should now work without crashing\n",
        "    test_outputs = model(test_images, test_questions, test_lengths)\n",
        "    print(f\"\\nTest forward pass successful!\")\n",
        "    print(f\"Output shape: {test_outputs.shape}\")"
      ],
      "metadata": {
        "id": "IJEL3eruT2Zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Calculate Class Weights\n",
        "# Count positive examples in training split\n",
        "num_pos = train_df['label'].sum()\n",
        "num_neg = len(train_df) - num_pos\n",
        "pos_weight = torch.tensor([num_neg / num_pos]).to(device)\n",
        "\n",
        "print(f\"Class Balance: Yes(1)={num_pos}, No(0)={num_neg}\")\n",
        "print(f\"Pos Weight: {pos_weight.item():.2f}\")\n",
        "\n",
        "# Loss & Optimizer\n",
        "# We use BCEWithLogitsLoss because it's numerically stable and handles pos_weight\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Metrics Helper\n",
        "def calculate_accuracy(preds, labels):\n",
        "    # Apply sigmoid here since model outputs logits\n",
        "    probs = torch.sigmoid(preds)\n",
        "    predicted_classes = (probs > 0.5).float()\n",
        "    correct = (predicted_classes == labels).float().sum()\n",
        "    return correct / len(labels)\n",
        "\n",
        "print(\"Training setup complete.\")"
      ],
      "metadata": {
        "id": "d7V3AKySmYhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Configuration**"
      ],
      "metadata": {
        "id": "H6-U8MULVA1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import time\n",
        "\n",
        "# CONFIGURATION\n",
        "NUM_EPOCHS = 20\n",
        "WARMUP_EPOCHS = 3  # Freeze backbone for first 3 epochs\n",
        "LEARNING_RATE_HEAD = 1e-3\n",
        "LEARNING_RATE_BACKBONE = 5e-5\n",
        "\n",
        "# Re-initialize Model fresh\n",
        "model = MultimodalVQAModel(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=300,\n",
        "    lstm_hidden_dim=64,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "# Initial Optimizer (Only training the head/LSTM params)\n",
        "# We filter for parameters that require grad\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LEARNING_RATE_HEAD)\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "print(f\"Training Strategy: {WARMUP_EPOCHS} Warmup Epochs -> Fine-Tuning\")\n",
        "\n",
        "# MAIN LOOP\n",
        "history = {'train_loss': [], 'val_acc': []}\n",
        "best_val_acc = 0.0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "\n",
        "    # AUTOMATIC UNFREEZING LOGIC\n",
        "    if epoch == WARMUP_EPOCHS:\n",
        "        print(\"\\nUNFREEZING BACKBONE (Fine-Tuning Mode Started)\")\n",
        "\n",
        "        # Unfreeze Vision Encoder\n",
        "        for param in model.vision_encoder.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        # Update Optimizer to include ALL parameters\n",
        "        #    Use different learning rates: small for ResNet, normal for Head\n",
        "        optimizer = optim.Adam([\n",
        "            {'params': model.vision_encoder.parameters(), 'lr': LEARNING_RATE_BACKBONE},\n",
        "            {'params': model.classifier.parameters(), 'lr': LEARNING_RATE_HEAD},\n",
        "            {'params': model.question_lstm.parameters(), 'lr': LEARNING_RATE_HEAD},\n",
        "            {'params': model.question_embedding.parameters(), 'lr': LEARNING_RATE_HEAD}\n",
        "        ])\n",
        "\n",
        "    # TRAIN\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, questions, lengths, labels in train_loader:\n",
        "        images, questions, labels = images.to(device), questions.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images, questions, lengths)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "    # VAL\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for images, questions, lengths, labels in val_loader:\n",
        "            images, questions, labels = images.to(device), questions.to(device), labels.to(device)\n",
        "            outputs = model(images, questions, lengths)\n",
        "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    # LOGGING\n",
        "    history['train_loss'].append(epoch_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Loss: {epoch_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), \"best_model.pth\")\n",
        "\n",
        "print(f\"\\nBest Accuracy: {best_val_acc:.4f}\")"
      ],
      "metadata": {
        "id": "rTnfPePL-Mhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ploting the Curves**"
      ],
      "metadata": {
        "id": "NHRscfcQTEPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc, classification_report\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# COMPREHENSIVE VISUALIZATION\n",
        "\n",
        "print(\"-\" * 40)\n",
        "print(\"GENERATING FINAL REPORT CHARTS\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Setup Data\n",
        "# Ensure we have the predictions from the Test Set\n",
        "model.eval()\n",
        "y_true = []\n",
        "y_scores = []\n",
        "y_preds = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, questions, lengths, labels in test_loader:\n",
        "        images, questions = images.to(device), questions.to(device)\n",
        "\n",
        "        # Forward\n",
        "        outputs = model(images, questions, lengths)\n",
        "        probs = torch.sigmoid(outputs)\n",
        "\n",
        "        y_scores.extend(probs.cpu().numpy())\n",
        "        y_preds.extend((probs > 0.5).float().cpu().numpy())\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "\n",
        "y_true = np.array(y_true)\n",
        "y_scores = np.array(y_scores)\n",
        "y_preds = np.array(y_preds)\n",
        "\n",
        "# Create Canvas (2x2 Grid)\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "plt.subplots_adjust(hspace=0.3, wspace=0.3)\n",
        "\n",
        "# Learning Curves (Top Left)\n",
        "# Robust check for available keys in history\n",
        "ax1 = axes[0, 0]\n",
        "if 'train_loss' in history:\n",
        "    ax1.plot(history['train_loss'], label='Train Loss', color='blue', linewidth=2)\n",
        "if 'val_loss' in history:\n",
        "    ax1.plot(history['val_loss'], label='Val Loss', color='orange', linestyle='--', linewidth=2)\n",
        "if 'val_acc' in history:\n",
        "    ax1_twin = ax1.twinx()\n",
        "    ax1_twin.plot(history['val_acc'], label='Val Acc', color='green', linestyle=':', linewidth=2)\n",
        "    ax1_twin.set_ylabel('Accuracy', color='green')\n",
        "\n",
        "ax1.set_title('Training Dynamics', fontsize=14, fontweight='bold')\n",
        "ax1.set_xlabel('Epochs')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend(loc='upper left')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "\n",
        "# Confusion Matrix (Top Right)\n",
        "ax2 = axes[0, 1]\n",
        "cm = confusion_matrix(y_true, y_preds)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'],\n",
        "            ax=ax2, annot_kws={\"size\": 16})\n",
        "ax2.set_title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
        "ax2.set_xlabel('Predicted Label')\n",
        "ax2.set_ylabel('True Label')\n",
        "\n",
        "\n",
        "# ROC Curve (Bottom Left)\n",
        "ax3 = axes[1, 0]\n",
        "fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "ax3.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "ax3.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "ax3.set_xlim([0.0, 1.0])\n",
        "ax3.set_ylim([0.0, 1.05])\n",
        "ax3.set_xlabel('False Positive Rate')\n",
        "ax3.set_ylabel('True Positive Rate')\n",
        "ax3.set_title('ROC Curve', fontsize=14, fontweight='bold')\n",
        "ax3.legend(loc=\"lower right\")\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "\n",
        "# Detailed Metrics (Bottom Right)\n",
        "ax4 = axes[1, 1]\n",
        "report = classification_report(y_true, y_preds, target_names=['No', 'Yes'], output_dict=True)\n",
        "metrics_df = pd.DataFrame(report).transpose().iloc[:2] # Get only Yes/No rows\n",
        "\n",
        "# Plot Bar Chart\n",
        "metrics_df[['precision', 'recall', 'f1-score']].plot(kind='bar', ax=ax4, color=['#a8dab5', '#4dbbd5', '#e64b35'])\n",
        "ax4.set_title('Per-Class Performance', fontsize=14, fontweight='bold')\n",
        "ax4.set_ylim(0, 1.0)\n",
        "ax4.legend(loc='lower center', ncol=3)\n",
        "ax4.grid(axis='y', alpha=0.3)\n",
        "ax4.tick_params(axis='x', rotation=0)\n",
        "\n",
        "# Show final plot\n",
        "plt.show()\n",
        "\n",
        "# Print Numeric Summary\n",
        "print(\"\\n\" + \"-\"*30)\n",
        "print(\"FINAL NUMERIC RESULTS\")\n",
        "print(\"-\"*30)\n",
        "print(classification_report(y_true, y_preds, target_names=['No', 'Yes']))\n",
        "print(f\"Global Test Accuracy: {np.mean(y_true == y_preds)*100:.2f}%\")\n",
        "print(f\"AUC Score: {roc_auc:.4f}\")"
      ],
      "metadata": {
        "id": "D-BbomJNTDmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc, classification_report\n",
        "\n",
        "# Ensure Drive folder exists\n",
        "DRIVE_FOLDER = \"/content/drive/MyDrive/VQA_Assignment_Results\"\n",
        "os.makedirs(DRIVE_FOLDER, exist_ok=True)\n",
        "print(f\"Saving individual charts to: {DRIVE_FOLDER}\")\n",
        "\n",
        "# SAVE QUANTITATIVE CHARTS INDIVIDUALLY\n",
        "\n",
        "# Learning Curves\n",
        "plt.figure(figsize=(8, 6))\n",
        "if 'train_loss' in history:\n",
        "    plt.plot(history['train_loss'], label='Train Loss', color='blue', linewidth=2)\n",
        "if 'val_loss' in history:\n",
        "    plt.plot(history['val_loss'], label='Val Loss', color='orange', linestyle='--', linewidth=2)\n",
        "plt.title('Training Loss Curve')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.savefig(os.path.join(DRIVE_FOLDER, \"chart_1_learning_curve.png\"), dpi=300)\n",
        "plt.close()\n",
        "print(\"  Saved chart_1_learning_curve.png\")\n",
        "\n",
        "# Confusion Matrix\n",
        "plt.figure(figsize=(6, 5))\n",
        "cm = confusion_matrix(y_true, y_preds)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'], annot_kws={\"size\": 14})\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.savefig(os.path.join(DRIVE_FOLDER, \"chart_2_confusion_matrix.png\"), dpi=300)\n",
        "plt.close()\n",
        "print(\"  Saved chart_2_confusion_matrix.png\")\n",
        "\n",
        "# ROC Curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'AUC = {roc_auc:.2f}')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.savefig(os.path.join(DRIVE_FOLDER, \"chart_3_roc_curve.png\"), dpi=300)\n",
        "plt.close()\n",
        "print(\"  Saved chart_3_roc_curve.png\")\n",
        "\n",
        "# Metrics Bar Chart\n",
        "plt.figure(figsize=(8, 6))\n",
        "report = classification_report(y_true, y_preds, target_names=['No', 'Yes'], output_dict=True)\n",
        "metrics_df = pd.DataFrame(report).transpose().iloc[:2]\n",
        "metrics_df[['precision', 'recall', 'f1-score']].plot(kind='bar', color=['#a8dab5', '#4dbbd5', '#e64b35'])\n",
        "plt.title('Precision, Recall & F1 Score')\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower center', ncol=3)\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.xticks(rotation=0)\n",
        "plt.savefig(os.path.join(DRIVE_FOLDER, \"chart_4_metrics_bar.png\"), dpi=300)\n",
        "plt.close()\n",
        "print(\"  Saved chart_4_metrics_bar.png\")\n",
        "\n",
        "# QUALITATIVE ANALYSIS (EXAMPLES)\n",
        "\n",
        "def get_text_from_tensor(question_tensor):\n",
        "    # Convert token IDs back to words\n",
        "    words = []\n",
        "    for idx in question_tensor:\n",
        "        idx = idx.item()\n",
        "        if idx == 0: break # Padding\n",
        "        # Find word for index\n",
        "        for word, vocab_idx in vocab.items():\n",
        "            if vocab_idx == idx:\n",
        "                words.append(word)\n",
        "                break\n",
        "    return \" \".join(words)\n",
        "\n",
        "# Get a batch of data\n",
        "model.eval()\n",
        "images, questions, lengths, labels = next(iter(test_loader))\n",
        "images, questions = images.to(device), questions.to(device)\n",
        "labels = labels.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(images, questions, lengths)\n",
        "    preds = (torch.sigmoid(outputs) > 0.5).float()\n",
        "\n",
        "# Find 3 Correct and 3 Incorrect\n",
        "correct_indices = (preds == labels).nonzero(as_tuple=True)[0].cpu().numpy()\n",
        "incorrect_indices = (preds != labels).nonzero(as_tuple=True)[0].cpu().numpy()\n",
        "\n",
        "# Plotting Helper\n",
        "def plot_examples(indices, title, filename, color):\n",
        "    if len(indices) < 3:\n",
        "        print(f\"Not enough {title} examples to plot.\")\n",
        "        return\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    for i, idx in enumerate(indices[:3]):\n",
        "        ax = axes[i]\n",
        "\n",
        "        # Un-normalize image for display\n",
        "        img = images[idx].cpu().permute(1, 2, 0).numpy()\n",
        "        img = img * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406] # Undo ImageNet Norm\n",
        "        img = np.clip(img, 0, 1)\n",
        "\n",
        "        ax.imshow(img)\n",
        "        ax.axis('off')\n",
        "\n",
        "        # Get Text\n",
        "        q_text = get_text_from_tensor(questions[idx])\n",
        "        pred_lbl = \"Yes\" if preds[idx] == 1 else \"No\"\n",
        "        true_lbl = \"Yes\" if labels[idx] == 1 else \"No\"\n",
        "\n",
        "        ax.set_title(f\"Q: {q_text}?\\nTrue: {true_lbl} | Pred: {pred_lbl}\",\n",
        "                     color=color, fontsize=11, fontweight='bold')\n",
        "\n",
        "    plt.suptitle(title, fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(DRIVE_FOLDER, filename), dpi=300)\n",
        "    plt.close()\n",
        "    print(f\"  Saved {filename}\")\n",
        "\n",
        "plot_examples(correct_indices, \"Correct Predictions (Success Cases)\", \"chart_5_success_cases.png\", \"green\")\n",
        "plot_examples(incorrect_indices, \"Incorrect Predictions (Failure Analysis)\", \"chart_6_failure_cases.png\", \"red\")"
      ],
      "metadata": {
        "id": "7gfWDl1wEPR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**BLIP Model Setup**"
      ],
      "metadata": {
        "id": "1QwTIlM0G-ed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# BLIP MODEL SETUP (Generative VQA)\n",
        "\n",
        "print(\"Installing Hugging Face Transformers...\")\n",
        "!pip install -q transformers\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BlipProcessor, BlipForQuestionAnswering\n",
        "from PIL import Image\n",
        "\n",
        "# Load BLIP Processor & Model\n",
        "# We use the 'base' model fine-tuned for VQA\n",
        "MODEL_ID = \"Salesforce/blip-vqa-base\"\n",
        "processor = BlipProcessor.from_pretrained(MODEL_ID)\n",
        "model = BlipForQuestionAnswering.from_pretrained(MODEL_ID).to(device)\n",
        "\n",
        "print(f\"Loaded: {MODEL_ID}\")\n",
        "\n",
        "# Define the BLIP Dataset\n",
        "class VQABlipDataset(Dataset):\n",
        "    def __init__(self, df, processor, image_dir):\n",
        "        self.df = df\n",
        "        self.processor = processor\n",
        "        self.image_dir = image_dir\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "\n",
        "        # Load Image\n",
        "        img_name = row['image_name']\n",
        "        img_path = f\"{self.image_dir}/{img_name}\"\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        # Get Text\n",
        "        question = row['question']\n",
        "        answer = row['answer'] # BLIP needs the actual word \"yes\" or \"no\"\n",
        "\n",
        "        # Process Inputs (Image + Question)\n",
        "        inputs = self.processor(\n",
        "            images=image,\n",
        "            text=question,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Process Targets (Answer)\n",
        "        targets = self.processor(\n",
        "            text=answer,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Format for Model\n",
        "        return {\n",
        "            \"pixel_values\": inputs.pixel_values.squeeze(),\n",
        "            \"input_ids\": inputs.input_ids.squeeze(),\n",
        "            \"labels\": targets.input_ids.squeeze()\n",
        "        }\n",
        "\n",
        "print(\"VQABlipDataset class defined.\")"
      ],
      "metadata": {
        "id": "gDC013A4Tguu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# PHASE 3: BLIP TRAINING\n",
        "\n",
        "# Custom Collate Function (Handles text padding)\n",
        "def blip_collate_fn(batch):\n",
        "    pixel_values = torch.stack([item['pixel_values'] for item in batch])\n",
        "\n",
        "    # Pad questions to the longest in the batch\n",
        "    input_ids = [item['input_ids'] for item in batch]\n",
        "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=processor.tokenizer.pad_token_id)\n",
        "\n",
        "    # Pad answers (labels) - use -100 to ignore padding in loss calculation\n",
        "    labels = [item['labels'] for item in batch]\n",
        "    labels = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
        "\n",
        "    return {\"pixel_values\": pixel_values, \"input_ids\": input_ids, \"labels\": labels}\n",
        "\n",
        "# Create DataLoaders\n",
        "# Create fresh datasets using the BLIP-specific class\n",
        "train_ds_blip = VQABlipDataset(train_df, processor, IMAGE_DIR)\n",
        "val_ds_blip = VQABlipDataset(val_df, processor, IMAGE_DIR)\n",
        "test_ds_blip = VQABlipDataset(test_df, processor, IMAGE_DIR)\n",
        "\n",
        "train_loader_blip = DataLoader(train_ds_blip, batch_size=8, shuffle=True, collate_fn=blip_collate_fn)\n",
        "val_loader_blip = DataLoader(val_ds_blip, batch_size=8, shuffle=False, collate_fn=blip_collate_fn)\n",
        "test_loader_blip = DataLoader(test_ds_blip, batch_size=8, shuffle=False, collate_fn=blip_collate_fn)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Evaluation Helper (Generative)\n",
        "def evaluate_blip(model, dataloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    print(\"  Validating (Generating answers)...\")\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            pixel_values = batch['pixel_values'].to(device)\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "\n",
        "            # Use BLIP to generate text answers\n",
        "            generated_ids = model.generate(pixel_values=pixel_values, input_ids=input_ids, max_new_tokens=10)\n",
        "            generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "            # Get Ground Truth (we need to decode the labels back to text)\n",
        "            label_ids = batch['labels'].cpu().numpy()\n",
        "            ground_truth_texts = []\n",
        "            for ids in label_ids:\n",
        "                # Filter out -100\n",
        "                ids = ids[ids != -100]\n",
        "                text = processor.decode(ids, skip_special_tokens=True)\n",
        "                ground_truth_texts.append(text)\n",
        "\n",
        "            # Compare\n",
        "            for pred, true in zip(generated_texts, ground_truth_texts):\n",
        "                # Simple normalization (lowercase, strip)\n",
        "                if pred.lower().strip() == true.lower().strip():\n",
        "                    correct += 1\n",
        "                total += 1\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "# Main Training Loop\n",
        "NUM_EPOCHS = 10\n",
        "best_acc = 0.0\n",
        "\n",
        "print(\"\\n\" + \"-\"*40)\n",
        "print(\"STARTING BLIP FINE-TUNING\")\n",
        "print(\"-\"*40)\n",
        "\n",
        "blip_history = {'loss': [], 'val_acc': []}\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # Train\n",
        "    for batch in tqdm(train_loader_blip, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\"):\n",
        "        pixel_values = batch['pixel_values'].to(device)\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward (BLIP computes loss automatically if labels are provided)\n",
        "        outputs = model(pixel_values=pixel_values, input_ids=input_ids, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader_blip)\n",
        "\n",
        "    # Validate\n",
        "    val_acc = evaluate_blip(model, val_loader_blip)\n",
        "\n",
        "    blip_history['loss'].append(avg_loss)\n",
        "    blip_history['val_acc'].append(val_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f} | Val Acc = {val_acc*100:.2f}%\")\n",
        "\n",
        "    # Save Best\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        torch.save(model.state_dict(), \"best_blip_model.pth\")\n",
        "        print(\"  New Best BLIP Model Saved!\")\n",
        "\n",
        "print(f\"\\nTraining Complete. Best Accuracy: {best_acc*100:.2f}%\")"
      ],
      "metadata": {
        "id": "L5EX-NCYTor1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# INSPECT BLIP PREDICTIONS\n",
        "\n",
        "model.eval()\n",
        "print(f\"Checking specific examples from Validation Set...\\n\")\n",
        "\n",
        "# Get a small batch\n",
        "batch = next(iter(val_loader_blip))\n",
        "pixel_values = batch['pixel_values'].to(device)\n",
        "input_ids = batch['input_ids'].to(device)\n",
        "labels = batch['labels']\n",
        "\n",
        "# Generate\n",
        "with torch.no_grad():\n",
        "    generated_ids = model.generate(pixel_values=pixel_values, input_ids=input_ids, max_new_tokens=5)\n",
        "    preds = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "# Decode Truth\n",
        "ground_truths = []\n",
        "for ids in labels:\n",
        "    ids = ids[ids != -100] # Remove padding\n",
        "    ground_truths.append(processor.decode(ids, skip_special_tokens=True))\n",
        "\n",
        "# Print Comparison\n",
        "print(f\"{'PREDICTION':<20} | {'TRUTH':<20} | {'STATUS'}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for p, t in zip(preds, ground_truths):\n",
        "    p_clean = p.strip().lower()\n",
        "    t_clean = t.strip().lower()\n",
        "    status = \"Correct\" if p_clean == t_clean else \"Wrong\"\n",
        "    print(f\"{p:<20} | {t:<20} | {status}\")"
      ],
      "metadata": {
        "id": "mPnTv8vfQUOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FINAL EVALUATION: BLIP MODEL\n",
        "\n",
        "# Load the best weights from training\n",
        "model.load_state_dict(torch.load(\"best_blip_model.pth\"))\n",
        "model.eval()\n",
        "\n",
        "print(\"Running Final Inference on TEST SET...\")\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader_blip):\n",
        "        pixel_values = batch['pixel_values'].to(device)\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        labels = batch['labels']\n",
        "\n",
        "        # Generate\n",
        "        generated_ids = model.generate(pixel_values=pixel_values, input_ids=input_ids, max_new_tokens=5)\n",
        "        preds_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "        # Store Preds\n",
        "        # Convert text \"yes\"/\"no\" back to 1/0 for metrics\n",
        "        for p in preds_text:\n",
        "            clean_p = p.strip().lower()\n",
        "            if \"yes\" in clean_p:\n",
        "                all_preds.append(1)\n",
        "            else:\n",
        "                all_preds.append(0) # Default to No if unsure\n",
        "\n",
        "        # Store Truth\n",
        "        for ids in labels:\n",
        "            ids = ids[ids != -100]\n",
        "            truth_text = processor.decode(ids, skip_special_tokens=True)\n",
        "            if \"yes\" in truth_text.lower():\n",
        "                all_labels.append(1)\n",
        "            else:\n",
        "                all_labels.append(0)\n",
        "\n",
        "# Calculate Metrics\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "blip_acc = accuracy_score(all_labels, all_preds)\n",
        "print(f\"\\nFINAL BLIP TEST ACCURACY: {blip_acc*100:.2f}%\")\n",
        "print(\"(Compare this to CNN: 56.93%)\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=['No', 'Yes']))\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
        "plt.title('Confusion Matrix (BLIP Transformer)')\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1sEHQGT3Qj9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define Results\n",
        "models = ['CNN Baseline', 'BLIP (VLP)']\n",
        "accuracies = [56.93, 65.69]\n",
        "f1_scores_yes = [0.56, 0.69] # From classification reports\n",
        "\n",
        "# Plot Side-by-Side Comparison\n",
        "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Accuracy Chart\n",
        "bars1 = ax[0].bar(models, accuracies, color=['#95a5a6', '#2ecc71'], width=0.5)\n",
        "ax[0].set_ylim(0, 100)\n",
        "ax[0].set_ylabel('Accuracy (%)')\n",
        "ax[0].set_title('Test Set Accuracy Comparison', fontsize=14, fontweight='bold')\n",
        "ax[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add numbers on top\n",
        "for bar in bars1:\n",
        "    yval = bar.get_height()\n",
        "    ax[0].text(bar.get_x() + bar.get_width()/2, yval + 1, f\"{yval}%\", ha='center', fontweight='bold')\n",
        "\n",
        "# F1-Score Chart (The \"Medical\" Metric)\n",
        "bars2 = ax[1].bar(models, f1_scores_yes, color=['#95a5a6', '#3498db'], width=0.5)\n",
        "ax[1].set_ylim(0, 1.0)\n",
        "ax[1].set_ylabel('F1-Score (Class: Yes)')\n",
        "ax[1].set_title('Ability to Detect Pathology (\"Yes\" Class)', fontsize=14, fontweight='bold')\n",
        "ax[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "for bar in bars2:\n",
        "    yval = bar.get_height()\n",
        "    ax[1].text(bar.get_x() + bar.get_width()/2, yval + 0.02, f\"{yval:.2f}\", ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save to Drive\n",
        "DRIVE_FOLDER = \"/content/drive/MyDrive/VQA_Assignment_Results\"\n",
        "if not os.path.exists(DRIVE_FOLDER):\n",
        "    os.makedirs(DRIVE_FOLDER)\n",
        "\n",
        "# Save Plot\n",
        "plot_path = os.path.join(DRIVE_FOLDER, \"final_comparison_chart.png\")\n",
        "fig.savefig(plot_path, dpi=300)\n",
        "print(f\"Saved Comparison Chart to: {plot_path}\")\n",
        "\n",
        "# Save BLIP Confusion Matrix (Regenerating to save)\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'], annot_kws={\"size\": 14})\n",
        "plt.title('Confusion Matrix (BLIP)')\n",
        "plt.savefig(os.path.join(DRIVE_FOLDER, \"blip_confusion_matrix.png\"), dpi=300)\n",
        "plt.close()\n",
        "print(f\"Saved BLIP Confusion Matrix\")\n",
        "\n",
        "# Save Best BLIP Model\n",
        "print(\"Backing up BLIP model weights (this might take 30s)...\")\n",
        "shutil.copy(\"best_blip_model.pth\", os.path.join(DRIVE_FOLDER, \"blip_best_model.pth\"))\n",
        "print(f\"Saved BLIP Model Weights to Drive\")\n",
        "\n",
        "print(\"\\nASSIGNMENT COMPLETE. You have all models, charts, and metrics secured.\")"
      ],
      "metadata": {
        "id": "lvDEZaO4Qq00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualizations**"
      ],
      "metadata": {
        "id": "8vj-toxj6qOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Setup Drive Path\n",
        "DRIVE_FOLDER = \"/content/drive/MyDrive/VQA_Assignment_Results\"\n",
        "os.makedirs(DRIVE_FOLDER, exist_ok=True)\n",
        "print(f\"Saving BLIP individual charts to: {DRIVE_FOLDER}\")\n",
        "\n",
        "# BLIP LEARNING CURVES (Fixed Legend)\n",
        "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Loss (Left Axis)\n",
        "#  plot explicitly on ax1\n",
        "line1 = ax1.plot(blip_history['loss'], label='Training Loss', color='#2ecc71', linewidth=2, marker='o', markersize=4)\n",
        "ax1.set_xlabel('Epochs')\n",
        "ax1.set_ylabel('Loss', color='#2ecc71', fontweight='bold')\n",
        "ax1.tick_params(axis='y', labelcolor='#2ecc71')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Accuracy (Right Axis)\n",
        "#  create a twin axis sharing the same x-axis\n",
        "ax2 = ax1.twinx()\n",
        "line2 = ax2.plot(blip_history['val_acc'], label='Val Accuracy', color='#e67e22', linestyle='--', linewidth=2, marker='s', markersize=4)\n",
        "ax2.set_ylabel('Accuracy', color='#e67e22', fontweight='bold')\n",
        "ax2.set_ylim(0, 1.0)\n",
        "ax2.tick_params(axis='y', labelcolor='#e67e22')\n",
        "\n",
        "# Title\n",
        "plt.title('BLIP Training Dynamics (Loss vs Accuracy)', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Combine Legends from both axes\n",
        "# Add the lines together so they appear in one box\n",
        "lines = line1 + line2\n",
        "labels = [l.get_label() for l in lines]\n",
        "ax1.legend(lines, labels, loc='center right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(DRIVE_FOLDER, \"blip_learning_curve.png\"), dpi=300)\n",
        "plt.show()\n",
        "print(\"Saved blip_learning_curve.png\")\n",
        "\n",
        "# BLIP CONFUSION MATRIX\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens',\n",
        "            xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'],\n",
        "            annot_kws={\"size\": 16})\n",
        "plt.title('Confusion Matrix (BLIP)', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(DRIVE_FOLDER, \"blip_confusion_matrix.png\"), dpi=300)\n",
        "plt.show()\n",
        "print(\"Saved blip_confusion_matrix.png\")\n",
        "\n",
        "# BLIP METRICS BAR CHART\n",
        "report = classification_report(all_labels, all_preds, target_names=['No', 'Yes'], output_dict=True)\n",
        "metrics_df = pd.DataFrame(report).transpose().iloc[:2]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "metrics_df[['precision', 'recall', 'f1-score']].plot(kind='bar', color=['#a8dab5', '#4dbbd5', '#e64b35'], figsize=(8,6))\n",
        "plt.title('BLIP Detailed Metrics', fontsize=14, fontweight='bold')\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower center', ncol=3)\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(DRIVE_FOLDER, \"blip_metrics_bar.png\"), dpi=300)\n",
        "plt.show()\n",
        "print(\"Saved blip_metrics_bar.png\")"
      ],
      "metadata": {
        "id": "SES4JlxOE1xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Final Comparison Visualization**"
      ],
      "metadata": {
        "id": "_JyYhfOARtzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Setup Drive Path\n",
        "DRIVE_FOLDER = \"/content/drive/MyDrive/VQA_Assignment_Results\"\n",
        "os.makedirs(DRIVE_FOLDER, exist_ok=True)\n",
        "\n",
        "# FINAL COMPARISON (CNN vs BLIP)\n",
        "\n",
        "# Define Data (Hardcoded from classification reports)\n",
        "# CNN: Prec=0.67, Rec=0.47, F1=0.56 | Acc=56.93\n",
        "# BLIP: Prec=0.71, Rec=0.68, F1=0.69 | Acc=65.69\n",
        "\n",
        "metrics_labels = ['Precision', 'Recall', 'F1-Score']\n",
        "cnn_metrics = [0.67, 0.47, 0.56]\n",
        "blip_metrics = [0.71, 0.68, 0.69]\n",
        "\n",
        "accuracies = [56.93, 65.69]\n",
        "model_names = ['CNN Baseline', 'BLIP (VLP)']\n",
        "\n",
        "# Create Detailed Comparison Table\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Metric': ['Test Accuracy', 'Precision (Yes)', 'Recall (Yes)', 'F1-Score (Yes)'],\n",
        "    'CNN Baseline': ['56.93%', '0.67', '0.47', '0.56'],\n",
        "    'BLIP (VLP)':   ['65.69%', '0.71', '0.68', '0.69'],\n",
        "    'Improvement':  ['+8.76%', '+0.04', '+0.21', '+0.13']\n",
        "})\n",
        "\n",
        "print(\"\\nFINAL PERFORMANCE COMPARISON TABLE\")\n",
        "print(\"-\"*55)\n",
        "print(comparison_df.to_string(index=False))\n",
        "print(\"-\"*55)\n",
        "\n",
        "# Save Table to CSV\n",
        "comparison_df.to_csv(os.path.join(DRIVE_FOLDER, \"final_comparison_table.csv\"), index=False)\n",
        "\n",
        "\n",
        "# Create Comparison Plots\n",
        "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Overall Accuracy\n",
        "bars1 = ax[0].bar(model_names, accuracies, color=['#95a5a6', '#2ecc71'], width=0.5)\n",
        "ax[0].set_ylim(0, 85)\n",
        "ax[0].set_ylabel('Accuracy (%)')\n",
        "ax[0].set_title('Overall Test Accuracy', fontsize=14, fontweight='bold')\n",
        "ax[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add labels\n",
        "for bar in bars1:\n",
        "    yval = bar.get_height()\n",
        "    ax[0].text(bar.get_x() + bar.get_width()/2, yval + 1, f\"{yval}%\", ha='center', fontweight='bold', fontsize=12)\n",
        "\n",
        "\n",
        "# Detailed Metrics (Grouped Bar Chart)\n",
        "x = np.arange(len(metrics_labels))  # Label locations\n",
        "width = 0.35  # Width of the bars\n",
        "\n",
        "rects1 = ax[1].bar(x - width/2, cnn_metrics, width, label='CNN Baseline', color='#95a5a6')\n",
        "rects2 = ax[1].bar(x + width/2, blip_metrics, width, label='BLIP (VLP)', color='#3498db')\n",
        "\n",
        "ax[1].set_ylabel('Score (0-1)')\n",
        "ax[1].set_title('Detailed Metrics for \"Yes\" Class (Pathology)', fontsize=14, fontweight='bold')\n",
        "ax[1].set_xticks(x)\n",
        "ax[1].set_xticklabels(metrics_labels, fontsize=11)\n",
        "ax[1].set_ylim(0, 1.0)\n",
        "ax[1].legend()\n",
        "ax[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add labels on top of bars\n",
        "def autolabel(rects):\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax[1].annotate(f'{height:.2f}',\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 3),  # 3 points vertical offset\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "autolabel(rects1)\n",
        "autolabel(rects2)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(DRIVE_FOLDER, \"final_comparison_chart_detailed.png\"), dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# Final Backup\n",
        "print(\"Backing up BLIP model weights...\")\n",
        "shutil.copy(\"best_blip_model.pth\", os.path.join(DRIVE_FOLDER, \"blip_best_model.pth\"))\n",
        "print(f\"Saved BLIP Model Weights and Final Charts to Drive.\")"
      ],
      "metadata": {
        "id": "w_5PP06URXXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kPysvEE57AcH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}